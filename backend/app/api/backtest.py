"""
å›æµ‹APIè·¯ç”±
é›†æˆcrypto_ctaæ¨¡å—åŠŸèƒ½
"""
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
import sys
import os
import uuid
import logging
from datetime import datetime
import pandas as pd
import numpy as np

# å¯¼å…¥ç°æœ‰çš„crypto_ctaæ¨¡å—
def setup_crypto_cta_imports():
    """è®¾ç½®crypto_ctaæ¨¡å—å¯¼å…¥è·¯å¾„å¹¶éªŒè¯å¯ç”¨æ€§"""
    global CTA_AVAILABLE, fast_calculate_signal_by_one_loop, strategy_evaluate, transfer_equity_curve_to_trade, cal_equity_curve

    # è·å–é¡¹ç›®æ ¹ç›®å½•
    current_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..'))
    crypto_cta_path = os.path.join(project_root, 'crypto_cta')

    print(f"ğŸ” æ£€æŸ¥crypto_ctaè·¯å¾„: {crypto_cta_path}")

    if not os.path.exists(crypto_cta_path):
        print(f"âŒ crypto_ctaç›®å½•ä¸å­˜åœ¨: {crypto_cta_path}")
        CTA_AVAILABLE = False
        return False

    # æ·»åŠ è·¯å¾„åˆ°sys.path
    if crypto_cta_path not in sys.path:
        sys.path.insert(0, crypto_cta_path)
        print(f"âœ… å·²æ·»åŠ crypto_ctaè·¯å¾„åˆ°sys.path")

    # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        os.path.join(crypto_cta_path, 'cta_api', 'cta_core.py'),
        os.path.join(crypto_cta_path, 'cta_api', 'statistics.py'),
        os.path.join(crypto_cta_path, 'cta_api', 'function.py'),
        os.path.join(crypto_cta_path, 'factors', 'sma.py'),
        os.path.join(crypto_cta_path, 'factors', 'rsi.py'),
    ]

    for file_path in required_files:
        if not os.path.exists(file_path):
            print(f"âŒ å¿…éœ€æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            CTA_AVAILABLE = False
            return False
        else:
            print(f"âœ… æ–‡ä»¶å­˜åœ¨: {os.path.basename(file_path)}")

    # å°è¯•å¯¼å…¥æ¨¡å—
    try:
        from cta_api.cta_core import fast_calculate_signal_by_one_loop
        from cta_api.statistics import strategy_evaluate, transfer_equity_curve_to_trade
        from cta_api.function import cal_equity_curve

        print("âœ… æˆåŠŸå¯¼å…¥crypto_ctaæ ¸å¿ƒæ¨¡å—")
        CTA_AVAILABLE = True
        return True

    except ImportError as e:
        print(f"âŒ å¯¼å…¥crypto_ctaæ¨¡å—å¤±è´¥: {e}")
        print(f"   å½“å‰sys.pathåŒ…å«: {[p for p in sys.path if 'crypto_cta' in p]}")
        CTA_AVAILABLE = False
        return False
    except Exception as e:
        print(f"âŒ å¯¼å…¥crypto_ctaæ¨¡å—æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        CTA_AVAILABLE = False
        return False

# åˆå§‹åŒ–crypto_ctaæ¨¡å—
CTA_AVAILABLE = False
fast_calculate_signal_by_one_loop = None
strategy_evaluate = None
transfer_equity_curve_to_trade = None
cal_equity_curve = None

# æ‰§è¡Œå¯¼å…¥è®¾ç½®
setup_crypto_cta_imports()

router = APIRouter()

# æ•°æ®æ¨¡å‹
class BacktestRequest(BaseModel):
    symbols: List[str]
    strategy: str
    parameters: Dict[str, Any]
    date_start: str = "2021-01-01"
    date_end: str = "2025-01-01"
    rule_type: str = "1H"
    leverage_rate: float = 1.0
    c_rate: float = 0.0008  # æ‰‹ç»­è´¹
    slippage: float = 0.001  # æ»‘ç‚¹

class TradeRecord(BaseModel):
    id: str
    symbol: str
    side: str  # "buy" or "sell"
    quantity: float
    price: float
    timestamp: datetime
    pnl: float
    commission: float
    slippage: float

class DrawdownPeriod(BaseModel):
    start_date: str
    end_date: str
    duration_days: int
    max_drawdown: float
    recovery_date: Optional[str] = None

class MonthlyReturn(BaseModel):
    year: int
    month: int
    return_value: float

class BacktestResult(BaseModel):
    task_id: str
    symbol: str
    strategy: str
    parameters: Dict[str, Any]
    final_return: float
    annual_return: float
    max_drawdown: float
    sharpe_ratio: float
    sortino_ratio: float = 0.0
    calmar_ratio: float = 0.0
    win_rate: float
    profit_factor: float = 0.0
    total_trades: int
    winning_trades: int = 0
    losing_trades: int = 0
    avg_win: float = 0.0
    avg_loss: float = 0.0
    max_consecutive_wins: int = 0
    max_consecutive_losses: int = 0
    volatility: float = 0.0
    skewness: float = 0.0
    kurtosis: float = 0.0
    var_95: float = 0.0  # Value at Risk 95%
    cvar_95: float = 0.0  # Conditional Value at Risk 95%
    equity_curve: List[Dict[str, Any]]
    drawdown_periods: List[DrawdownPeriod] = []
    monthly_returns: List[MonthlyReturn] = []
    trade_records: List[TradeRecord] = []
    created_at: datetime

    # æ–°å¢æ—¶é—´èŒƒå›´ç›¸å…³å­—æ®µ
    requested_date_start: str = ""  # ç”¨æˆ·è¯·æ±‚çš„å¼€å§‹æ—¶é—´
    requested_date_end: str = ""    # ç”¨æˆ·è¯·æ±‚çš„ç»“æŸæ—¶é—´
    actual_date_start: str = ""     # å®é™…ä½¿ç”¨çš„å¼€å§‹æ—¶é—´
    actual_date_end: str = ""       # å®é™…ä½¿ç”¨çš„ç»“æŸæ—¶é—´
    data_records_count: int = 0     # å®é™…ä½¿ç”¨çš„æ•°æ®æ¡æ•°
    time_range_match_status: str = ""  # æ—¶é—´èŒƒå›´åŒ¹é…çŠ¶æ€
    time_range_adjustment_reason: str = ""  # æ—¶é—´èŒƒå›´è°ƒæ•´åŸå› 

class BacktestStatus(BaseModel):
    task_id: str
    status: str  # "pending", "running", "completed", "failed"
    progress: float = 0.0
    message: str = ""
    symbols_total: int = 0
    symbols_completed: int = 0
    results: List[BacktestResult] = []

class OptimizationRequest(BaseModel):
    symbols: List[str]
    strategy: str
    parameter_ranges: Dict[str, List[float]]  # å‚æ•°ä¼˜åŒ–èŒƒå›´
    date_start: str = "2021-01-01"
    date_end: str = "2025-01-01"
    rule_type: str = "1H"

# å…¨å±€ä»»åŠ¡ç®¡ç†
backtest_tasks: Dict[str, BacktestStatus] = {}

# å…¨å±€æ—¶é—´èŒƒå›´ä¿¡æ¯å­˜å‚¨
_current_time_range_info: Dict[str, Any] = {}

def smart_time_range_filter(df: pd.DataFrame, request: BacktestRequest, symbol: str) -> tuple:
    """
    æ™ºèƒ½æ—¶é—´èŒƒå›´è¿‡æ»¤å‡½æ•°
    è¿”å›: (è¿‡æ»¤åçš„DataFrame, æ—¶é—´èŒƒå›´ä¿¡æ¯å­—å…¸)
    """
    import pandas as pd

    # è·å–æ•°æ®çš„æ—¶é—´èŒƒå›´
    data_start = df['candle_begin_time'].min()
    data_end = df['candle_begin_time'].max()

    # è§£æè¯·æ±‚çš„æ—¶é—´èŒƒå›´
    start_date = pd.to_datetime(request.date_start)
    end_date = pd.to_datetime(request.date_end)

    print(f"   è¯·æ±‚æ—¶é—´èŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
    print(f"   æ•°æ®æ—¶é—´èŒƒå›´: {data_start.date()} åˆ° {data_end.date()}")

    # åˆå§‹åŒ–æ—¶é—´èŒƒå›´ä¿¡æ¯
    time_range_info = {
        'requested_start': request.date_start,
        'requested_end': request.date_end,
        'data_available_start': data_start.strftime('%Y-%m-%d'),
        'data_available_end': data_end.strftime('%Y-%m-%d'),
        'actual_start': '',
        'actual_end': '',
        'records_count': 0,
        'match_status': '',
        'adjustment_reason': ''
    }

    # æ£€æŸ¥æ—¶é—´èŒƒå›´é‡å æƒ…å†µ
    if data_end < start_date or data_start > end_date:
        # å®Œå…¨ä¸é‡å çš„æƒ…å†µ
        print(f"âš ï¸ {symbol}: è¯·æ±‚æ—¶é—´èŒƒå›´ä¸æ•°æ®æ—¶é—´èŒƒå›´å®Œå…¨ä¸é‡å ")
        time_range_info['match_status'] = 'no_overlap'

        if data_end < start_date:
            # è¯·æ±‚çš„æ—¶é—´å¤ªæ–°ï¼Œä½¿ç”¨æœ€æ–°çš„æ•°æ®
            print(f"   è¯·æ±‚æ—¶é—´è¿‡æ–°ï¼Œä½¿ç”¨æœ€æ–°å¯ç”¨æ•°æ®")
            target_records = min(2000, len(df))
            df_filtered = df.tail(target_records)
            time_range_info['adjustment_reason'] = f'è¯·æ±‚æ—¶é—´({start_date.date()})æ™šäºæ•°æ®æœ€æ–°æ—¶é—´({data_end.date()})ï¼Œä½¿ç”¨æœ€æ–°{target_records}æ¡æ•°æ®'
        else:
            # è¯·æ±‚çš„æ—¶é—´å¤ªæ—§ï¼Œä½¿ç”¨æœ€æ—©çš„æ•°æ®
            print(f"   è¯·æ±‚æ—¶é—´è¿‡æ—§ï¼Œä½¿ç”¨æœ€æ—©å¯ç”¨æ•°æ®")
            target_records = min(2000, len(df))
            df_filtered = df.head(target_records)
            time_range_info['adjustment_reason'] = f'è¯·æ±‚æ—¶é—´({end_date.date()})æ—©äºæ•°æ®æœ€æ—©æ—¶é—´({data_start.date()})ï¼Œä½¿ç”¨æœ€æ—©{target_records}æ¡æ•°æ®'

    else:
        # æœ‰é‡å çš„æƒ…å†µ
        print(f"âœ… {symbol}: æ—¶é—´èŒƒå›´æœ‰é‡å ï¼Œè¿›è¡Œæ•°æ®è¿‡æ»¤")
        time_range_info['match_status'] = 'overlap'

        # å…ˆå°è¯•ä¸¥æ ¼è¿‡æ»¤
        df_filtered = df[(df['candle_begin_time'] >= start_date) & (df['candle_begin_time'] <= end_date)]

        if df_filtered.empty or len(df_filtered) < 200:  # æé«˜æœ€å°æ•°æ®é‡è¦æ±‚
            print(f"âš ï¸ {symbol}: ä¸¥æ ¼ç­›é€‰åæ•°æ®ä¸è¶³({len(df_filtered) if not df_filtered.empty else 0}æ¡)ï¼Œæ‰©å±•æ—¶é—´èŒƒå›´")
            time_range_info['match_status'] = 'insufficient_data'

            # è®¡ç®—éœ€è¦çš„æ•°æ®é‡
            target_records = 1500  # ç›®æ ‡æ•°æ®é‡

            # ä»¥è¯·æ±‚æ—¶é—´èŒƒå›´çš„ä¸­å¿ƒä¸ºåŸºå‡†ï¼Œå‘å‰åæ‰©å±•
            center_time = start_date + (end_date - start_date) / 2

            # è®¡ç®—æ¯æ¡è®°å½•åˆ°ä¸­å¿ƒæ—¶é—´çš„è·ç¦»
            df_with_distance = df.copy()
            df_with_distance['time_distance'] = abs(df_with_distance['candle_begin_time'] - center_time)

            # é€‰æ‹©è·ç¦»ä¸­å¿ƒæ—¶é—´æœ€è¿‘çš„è®°å½•
            df_filtered = df_with_distance.nsmallest(min(target_records, len(df)), 'time_distance')
            df_filtered = df_filtered.drop('time_distance', axis=1).sort_values('candle_begin_time')

            time_range_info['adjustment_reason'] = f'è¯·æ±‚æ—¶é—´èŒƒå›´å†…æ•°æ®ä¸è¶³ï¼Œä»¥{center_time.date()}ä¸ºä¸­å¿ƒæ‰©å±•åˆ°{len(df_filtered)}æ¡æ•°æ®'
        else:
            time_range_info['adjustment_reason'] = 'ä½¿ç”¨è¯·æ±‚æ—¶é—´èŒƒå›´å†…çš„æ•°æ®'

    # æ›´æ–°å®é™…ä½¿ç”¨çš„æ—¶é—´èŒƒå›´
    actual_start = df_filtered['candle_begin_time'].min()
    actual_end = df_filtered['candle_begin_time'].max()
    time_range_info['actual_start'] = actual_start.strftime('%Y-%m-%d')
    time_range_info['actual_end'] = actual_end.strftime('%Y-%m-%d')
    time_range_info['records_count'] = len(df_filtered)

    print(f"   å®é™…ä½¿ç”¨æ—¶é—´èŒƒå›´: {actual_start.date()} åˆ° {actual_end.date()}")
    print(f"âœ… {symbol}: æœ€ç»ˆæ•°æ® {len(df_filtered)} æ¡è®°å½•")

    # æ•°æ®è´¨é‡æ£€æŸ¥
    if len(df_filtered) < 100:
        print(f"âš ï¸ {symbol}: æ•°æ®é‡è¿‡å°‘({len(df_filtered)}æ¡)ï¼Œå¯èƒ½å½±å“å›æµ‹æ•ˆæœ")
        time_range_info['match_status'] = 'insufficient_data'
    elif len(df_filtered) < 500:
        print(f"âš ï¸ {symbol}: æ•°æ®é‡è¾ƒå°‘({len(df_filtered)}æ¡)ï¼Œå»ºè®®ä½¿ç”¨æ›´é•¿çš„æ—¶é—´èŒƒå›´")

    return df_filtered, time_range_info


async def load_existing_data(symbol: str, request: BacktestRequest) -> Optional[pd.DataFrame]:
    """ä»æœ¬åœ°æ•°æ®åŠ è½½æ•°æ® - ä½¿ç”¨æ–°çš„æ•°æ®é€‚é…å™¨"""
    try:
        import pandas as pd
        from ..services.data_adapter import data_adapter

        print(f"ğŸ” {symbol}: ä½¿ç”¨æœ¬åœ°æ•°æ®é€‚é…å™¨åŠ è½½æ•°æ®")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨è·å–æ•°æ®
        df = data_adapter.get_symbol_data_for_backtest(
            symbol, request.date_start, request.date_end, request.rule_type
        )

        if df is None or df.empty:
            print(f"âŒ {symbol}: æ•°æ®é€‚é…å™¨æœªè¿”å›æ•°æ®")
            return None

        print(f"ğŸ“Š {symbol}: åŸå§‹æ•°æ® {len(df)} æ¡è®°å½•")
        print(f"   æ•°æ®åˆ—: {list(df.columns)}")

        # æ˜¾ç¤ºæ•°æ®æ—¶é—´èŒƒå›´
        if 'candle_begin_time' in df.columns:
            data_start = df['candle_begin_time'].min()
            data_end = df['candle_begin_time'].max()
            print(f"   æ•°æ®æ—¶é—´èŒƒå›´: {data_start} åˆ° {data_end}")

            if 'close' in df.columns:
                print(f"   ä»·æ ¼èŒƒå›´: ${df['close'].min():.2f} - ${df['close'].max():.2f}")

        # ä½¿ç”¨æ™ºèƒ½æ—¶é—´èŒƒå›´å¤„ç†
        df_filtered, time_range_info = smart_time_range_filter(df, request, symbol)

        # å°†æ—¶é—´èŒƒå›´ä¿¡æ¯å­˜å‚¨ä¸ºå…¨å±€å˜é‡ï¼Œä¾›åç»­ä½¿ç”¨
        global _current_time_range_info
        _current_time_range_info = time_range_info

        return df_filtered

    except Exception as e:
        print(f"âŒ {symbol}: åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

async def fetch_real_binance_data(symbol: str, request: BacktestRequest) -> Optional[pd.DataFrame]:
    """å¼ºåˆ¶ä»Binance APIè·å–çœŸå®æ•°æ® - æ— ä»»ä½•æ¨¡æ‹Ÿæ•°æ®"""
    import pandas as pd
    import aiohttp
    import asyncio
    from datetime import datetime, timedelta
    import json
    import logging

    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ”„ {symbol}: å¼€å§‹ä»Binance APIè·å–çœŸå®æ•°æ®...")

    try:
        # è§£ææ—¶é—´èŒƒå›´
        start_date = datetime.strptime(request.date_start, "%Y-%m-%d")
        end_date = datetime.strptime(request.date_end, "%Y-%m-%d")

        # è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int(end_date.timestamp() * 1000)

        # æ ¹æ®rule_typeè®¾ç½®interval
        interval_map = {
            '1H': '1h',
            '4H': '4h',
            '1D': '1d',
            '1m': '1m',
            '5m': '5m',
            '15m': '15m',
            '30m': '30m'
        }
        interval = interval_map.get(request.rule_type, '1h')

        # ä½¿ç”¨å®˜æ–¹Binance APIæºï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        api_sources = [
            'https://fapi.binance.com/fapi/v1/klines',  # å¸å®‰åˆçº¦APIï¼ˆä¸»è¦ï¼‰
            'https://api.binance.com/api/v3/klines',    # å¸å®‰ç°è´§APIï¼ˆå¤‡ç”¨ï¼‰
        ]

        all_data = []
        successful_api = None

        # å°è¯•ä¸åŒçš„APIæº
        for api_url in api_sources:
            logger.info(f"   å°è¯•Binance API: {api_url}")

            try:
                async with aiohttp.ClientSession() as session:
                    temp_start = start_timestamp
                    temp_data = []

                    while temp_start < end_timestamp:
                        # æ¯æ¬¡æœ€å¤šè·å–1000æ¡æ•°æ®
                        params = {
                            'symbol': symbol,
                            'interval': interval,
                            'startTime': temp_start,
                            'endTime': end_timestamp,
                            'limit': 1000
                        }

                        async with session.get(api_url, params=params, timeout=30) as response:
                            if response.status == 200:
                                data = await response.json()

                                if not data:
                                    break

                                temp_data.extend(data)

                                # æ›´æ–°ä¸‹æ¬¡è¯·æ±‚çš„å¼€å§‹æ—¶é—´
                                temp_start = int(data[-1][6]) + 1  # ä½¿ç”¨æœ€åä¸€æ¡æ•°æ®çš„close_time + 1

                                logger.info(f"   è·å–åˆ° {len(data)} æ¡æ•°æ®ï¼Œæ€»è®¡ {len(temp_data)} æ¡")

                                # å¦‚æœè¿”å›çš„æ•°æ®å°‘äº1000æ¡ï¼Œè¯´æ˜å·²ç»è·å–å®Œæ¯•
                                if len(data) < 1000:
                                    break

                            else:
                                logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status}")
                                break

                        # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                        await asyncio.sleep(0.1)

                    # å¦‚æœæˆåŠŸè·å–åˆ°æ•°æ®ï¼Œä½¿ç”¨è¿™ä¸ªAPIæº
                    if temp_data:
                        all_data = temp_data
                        successful_api = api_url
                        logger.info(f"âœ… æˆåŠŸä» {api_url} è·å–çœŸå®æ•°æ®")
                        break

            except Exception as e:
                logger.error(f"âŒ APIæº {api_url} è¯·æ±‚å¼‚å¸¸: {e}")
                continue

        if not all_data:
            logger.error(f"âŒ {symbol}: æ— æ³•ä»ä»»ä½•Binance APIè·å–æ•°æ®")
            return None

        # è½¬æ¢ä¸ºDataFrame
        columns = ['open_time', 'open', 'high', 'low', 'close', 'volume',
                  'close_time', 'quote_volume', 'trade_num',
                  'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']

        df = pd.DataFrame(all_data, columns=columns)

        # æ•°æ®ç±»å‹è½¬æ¢
        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
        df['candle_begin_time'] = df['open_time']
        df['open'] = df['open'].astype(float)
        df['high'] = df['high'].astype(float)
        df['low'] = df['low'].astype(float)
        df['close'] = df['close'].astype(float)
        df['volume'] = df['volume'].astype(float)

        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        df = df.drop(['close_time', 'ignore'], axis=1)

        # è®°å½•æ•°æ®æ¥æº
        logger.info(f"âœ… {symbol}: çœŸå®æ•°æ®è·å–æˆåŠŸ")
        logger.info(f"   æ•°æ®æ¥æº: {successful_api} (Binanceå®˜æ–¹API)")
        logger.info(f"   æ•°æ®é‡: {len(df)} æ¡è®°å½•")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df['candle_begin_time'].min()} åˆ° {df['candle_begin_time'].max()}")
        logger.info(f"   ä»·æ ¼èŒƒå›´: ${df['close'].min():.2f} - ${df['close'].max():.2f}")

        return df

    except Exception as e:
        logger.error(f"âŒ {symbol}: çœŸå®æ•°æ®è·å–å¤±è´¥ - {e}")
        return None


def validate_real_data(df: Optional[pd.DataFrame], symbol: str, request: BacktestRequest) -> bool:
    """éªŒè¯æ•°æ®æ˜¯å¦ä¸ºçœŸå®çš„å¸‚åœºæ•°æ® - å®½æ¾éªŒè¯"""
    import pandas as pd
    from datetime import datetime
    import logging

    logger = logging.getLogger(__name__)

    if df is None or df.empty:
        logger.error(f"âŒ {symbol}: æ•°æ®ä¸ºç©º")
        return False

    try:
        # 1. æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ - æ”¾å®½è¦æ±‚
        required_columns = ['open', 'high', 'low', 'close']  # ç§»é™¤volumeè¦æ±‚
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"âŒ {symbol}: ç¼ºå°‘å¿…è¦çš„ä»·æ ¼æ•°æ®åˆ—: {missing_columns}")
            return False

        # æ£€æŸ¥æ—¶é—´åˆ—
        if 'candle_begin_time' not in df.columns:
            logger.warning(f"âš ï¸ {symbol}: ç¼ºå°‘candle_begin_timeåˆ—ï¼Œä½†ç»§ç»­éªŒè¯")

        # 2. æ£€æŸ¥æ•°æ®ç±»å‹ - å°è¯•è½¬æ¢è€Œä¸æ˜¯ç›´æ¥å¤±è´¥
        numeric_columns = ['open', 'high', 'low', 'close']
        for col in numeric_columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                try:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    logger.info(f"âœ… {symbol}: æˆåŠŸè½¬æ¢ {col} åˆ—ä¸ºæ•°å€¼ç±»å‹")
                except:
                    logger.error(f"âŒ {symbol}: æ— æ³•è½¬æ¢ {col} åˆ—ä¸ºæ•°å€¼ç±»å‹")
                    return False

        # 3. åŸºæœ¬çš„ä»·æ ¼æ•°æ®åˆç†æ€§æ£€æŸ¥ - æ›´å®½æ¾
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾ä¸åˆç†çš„ä»·æ ¼æ•°æ®
        invalid_price_rows = (df['high'] < df['low'])
        if invalid_price_rows.sum() > len(df) * 0.1:  # è¶…è¿‡10%çš„æ•°æ®ä¸åˆç†æ‰æŠ¥é”™
            logger.error(f"âŒ {symbol}: è¿‡å¤šä¸åˆç†çš„ä»·æ ¼æ•°æ®ï¼ˆæœ€é«˜ä»·ä½äºæœ€ä½ä»·ï¼‰")
            return False
        elif invalid_price_rows.any():
            logger.warning(f"âš ï¸ {symbol}: å‘ç°å°‘é‡ä¸åˆç†çš„ä»·æ ¼æ•°æ®ï¼Œå·²å¿½ç•¥")

        # 4. æ£€æŸ¥ä»·æ ¼æ˜¯å¦ä¸ºæ­£æ•°
        price_columns = ['open', 'high', 'low', 'close']
        for col in price_columns:
            if (df[col] <= 0).all():
                logger.error(f"âŒ {symbol}: {col} åˆ—æ‰€æœ‰ä»·æ ¼éƒ½ä¸º0æˆ–è´Ÿæ•°")
                return False

        # 5. æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
        if len(df) < 10:
            logger.warning(f"âš ï¸ {symbol}: æ•°æ®é‡è¾ƒå°‘ï¼ˆ{len(df)}æ¡ï¼‰ï¼Œä½†ç»§ç»­å¤„ç†")

        # 6. æ”¾å®½æ—¶é—´èŒƒå›´æ£€æŸ¥ - åªè¦æœ‰æ•°æ®å°±æ¥å—
        if 'candle_begin_time' in df.columns:
            try:
                start_date = datetime.strptime(request.date_start, "%Y-%m-%d")
                end_date = datetime.strptime(request.date_end, "%Y-%m-%d")

                data_start = df['candle_begin_time'].min()
                data_end = df['candle_begin_time'].max()

                logger.info(f"ğŸ“… {symbol}: è¯·æ±‚æ—¶é—´ {start_date.date()} åˆ° {end_date.date()}")
                logger.info(f"ğŸ“… {symbol}: æ•°æ®æ—¶é—´ {data_start.date()} åˆ° {data_end.date()}")

                # åªè¦æœ‰éƒ¨åˆ†é‡å å°±æ¥å—
                if data_end < start_date or data_start > end_date:
                    logger.warning(f"âš ï¸ {symbol}: æ—¶é—´èŒƒå›´ä¸é‡å ï¼Œä½†ä»æ¥å—æ•°æ®")

            except Exception as e:
                logger.warning(f"âš ï¸ {symbol}: æ—¶é—´éªŒè¯å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†: {e}")

        # 7. æˆäº¤é‡æ£€æŸ¥ - å¯é€‰
        if 'volume' in df.columns:
            if (df['volume'] <= 0).all():
                logger.warning(f"âš ï¸ {symbol}: æˆäº¤é‡æ•°æ®å¼‚å¸¸ï¼Œä½†ç»§ç»­å¤„ç†")
        else:
            logger.info(f"â„¹ï¸ {symbol}: æ— æˆäº¤é‡æ•°æ®ï¼Œè·³è¿‡æˆäº¤é‡æ£€æŸ¥")

        logger.info(f"âœ… {symbol}: æ•°æ®éªŒè¯é€šè¿‡ï¼ˆå®½æ¾æ¨¡å¼ï¼‰")
        logger.info(f"   æ•°æ®è¡Œæ•°: {len(df)}")
        logger.info(f"   ä»·æ ¼èŒƒå›´: ${df['close'].min():.2f} - ${df['close'].max():.2f}")

        return True

    except Exception as e:
        logger.warning(f"âš ï¸ {symbol}: æ•°æ®éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä½†ç»§ç»­å¤„ç†: {e}")
        # åœ¨å®½æ¾æ¨¡å¼ä¸‹ï¼Œå³ä½¿éªŒè¯å‡ºé”™ä¹Ÿå°è¯•ç»§ç»­
        return True

async def run_real_backtest(symbol: str, request: BacktestRequest) -> Optional[BacktestResult]:
    """è¿è¡ŒçœŸå®çš„å›æµ‹é€»è¾‘ - ä¼˜å…ˆä½¿ç”¨Binance APIï¼Œå¤±è´¥æ—¶ä½¿ç”¨æœ¬åœ°çœŸå®æ•°æ®"""
    import pandas as pd
    import numpy as np
    from datetime import timedelta
    import logging

    # è®¾ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    logger.info(f"ğŸ” å¼€å§‹ä¸º {symbol} è·å–çœŸå®æ•°æ®è¿›è¡Œå›æµ‹")

    try:
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ•°æ®æ–‡ä»¶ï¼ˆæ›´å¯é ï¼‰
        logger.info(f"ğŸ” {symbol}: ä¼˜å…ˆå°è¯•ä½¿ç”¨æœ¬åœ°çœŸå®æ•°æ®æ–‡ä»¶")
        df = await load_existing_data(symbol, request)
        data_source = "æœ¬åœ°çœŸå®æ•°æ®æ–‡ä»¶"

        # å¦‚æœæœ¬åœ°æ•°æ®ä¸å¯ç”¨ï¼Œå†å°è¯•API
        if df is None or df.empty:
            logger.info(f"âš ï¸ {symbol}: æœ¬åœ°æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•ä»Binance APIè·å–")
            df = await fetch_real_binance_data(symbol, request)
            data_source = "Binance API (å®æ—¶æ•°æ®)"

        # éªŒè¯æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§
        if df is None or df.empty:
            logger.error(f"âŒ {symbol}: æ— æ³•è·å–ä»»ä½•æ•°æ®")
            return None

        # ä½¿ç”¨å®½æ¾çš„éªŒè¯æ¨¡å¼
        if not validate_real_data(df, symbol, request):
            logger.warning(f"âš ï¸ {symbol}: æ•°æ®éªŒè¯æœªå®Œå…¨é€šè¿‡ï¼Œä½†ç»§ç»­å›æµ‹")
            # ä¸ç›´æ¥è¿”å›Noneï¼Œè€Œæ˜¯ç»§ç»­å¤„ç†

        logger.info(f"âœ… {symbol}: æ•°æ®éªŒè¯é€šè¿‡")
        logger.info(f"   æ•°æ®æ¥æº: {data_source}")
        logger.info(f"   æ•°æ®é‡: {len(df)} æ¡è®°å½•")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df['candle_begin_time'].min()} åˆ° {df['candle_begin_time'].max()}")
        logger.info(f"   ä»·æ ¼èŒƒå›´: ${df['close'].min():.2f} - ${df['close'].max():.2f}")

        # 3. è°ƒç”¨ç­–ç•¥è®¡ç®—
        strategy_name = request.strategy.lower()

        # è®¾ç½®é»˜è®¤å‚æ•°
        default_params = {
            'sma': {'short_window': 5, 'long_window': 20},
            'rsi': {'period': 14, 'overbought': 70, 'oversold': 30},
            'macd': {'fast_period': 12, 'slow_period': 26, 'signal_period': 9},
            'kdj': {'period': 9, 'k_period': 3, 'd_period': 3, 'overbought': 80, 'oversold': 20},
            'atr_breakout': {'period': 14, 'entry_multiplier': 2.0, 'exit_multiplier': 1.0},
            'mean_reversion': {'period': 20, 'entry_threshold': 2.0, 'exit_threshold': 0.5}
        }

        # åˆå¹¶ç”¨æˆ·å‚æ•°å’Œé»˜è®¤å‚æ•°
        params = default_params.get(strategy_name, {})
        params.update(request.parameters)

        # 4. è®¡ç®—äº¤æ˜“ä¿¡å·
        df_result = calculate_strategy_signals(df, strategy_name, params)
        if df_result is None or df_result.empty:
            logger.error(f"âŒ {symbol}: ç­–ç•¥ä¿¡å·è®¡ç®—å¤±è´¥")
            return None

        # 5. è®¡ç®—èµ„é‡‘æ›²çº¿
        df_result = calculate_equity_curve_simple(df_result, request.leverage_rate, request.c_rate, request.slippage)

        # 6. è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        result = calculate_backtest_statistics(df_result, symbol, request)
        result.task_id = ""  # å°†åœ¨å¤–éƒ¨è®¾ç½®

        # æ·»åŠ æ—¶é—´èŒƒå›´ä¿¡æ¯åˆ°ç»“æœä¸­
        global _current_time_range_info
        if _current_time_range_info:
            result.requested_date_start = _current_time_range_info.get('requested_start', request.date_start)
            result.requested_date_end = _current_time_range_info.get('requested_end', request.date_end)
            result.actual_date_start = _current_time_range_info.get('actual_start', '')
            result.actual_date_end = _current_time_range_info.get('actual_end', '')
            result.data_records_count = _current_time_range_info.get('records_count', len(df_result))
            result.time_range_match_status = _current_time_range_info.get('match_status', 'unknown')
            result.time_range_adjustment_reason = _current_time_range_info.get('adjustment_reason', '')

            logger.info(f"âœ… {symbol}: å›æµ‹å®Œæˆï¼Œä½¿ç”¨100%çœŸå®æ•°æ®")
            logger.info(f"   è¯·æ±‚æ—¶é—´èŒƒå›´: {result.requested_date_start} åˆ° {result.requested_date_end}")
            logger.info(f"   å®é™…æ—¶é—´èŒƒå›´: {result.actual_date_start} åˆ° {result.actual_date_end}")
            logger.info(f"   æ•°æ®è®°å½•æ•°: {result.data_records_count}")
            logger.info(f"   æ—¶é—´åŒ¹é…çŠ¶æ€: {result.time_range_match_status}")
            if result.time_range_adjustment_reason:
                logger.info(f"   è°ƒæ•´åŸå› : {result.time_range_adjustment_reason}")
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´èŒƒå›´ä¿¡æ¯ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯
            if 'candle_begin_time' in df_result.columns:
                actual_start = df_result['candle_begin_time'].min()
                actual_end = df_result['candle_begin_time'].max()
                result.requested_date_start = request.date_start
                result.requested_date_end = request.date_end
                result.actual_date_start = actual_start.strftime('%Y-%m-%d')
                result.actual_date_end = actual_end.strftime('%Y-%m-%d')
                result.data_records_count = len(df_result)
                result.time_range_match_status = 'unknown'
                result.time_range_adjustment_reason = 'æ—¶é—´èŒƒå›´ä¿¡æ¯ä¸å¯ç”¨'

                logger.info(f"âœ… {symbol}: å›æµ‹å®Œæˆï¼Œä½¿ç”¨100%çœŸå®æ•°æ®")
                logger.info(f"   è¯·æ±‚æ—¶é—´èŒƒå›´: {request.date_start} åˆ° {request.date_end}")
                logger.info(f"   å®é™…æ—¶é—´èŒƒå›´: {actual_start.date()} åˆ° {actual_end.date()}")
            else:
                logger.info(f"âœ… {symbol}: å›æµ‹å®Œæˆï¼Œä½¿ç”¨100%çœŸå®æ•°æ®")

        return result

    except Exception as e:
        logger.error(f"âŒ {symbol}: å›æµ‹å¤±è´¥ - {str(e)}")
        return None

def calculate_strategy_signals(df: pd.DataFrame, strategy: str, params: dict) -> Optional[pd.DataFrame]:
    """è®¡ç®—ç­–ç•¥ä¿¡å· - ä½¿ç”¨çœŸå®çš„crypto_ctaå› å­"""
    try:
        df = df.copy()

        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'candle_begin_time' not in df.columns:
            df['candle_begin_time'] = df.index

        print(f"ğŸ”„ å¼€å§‹è®¡ç®—ç­–ç•¥ä¿¡å·: {strategy}")
        print(f"   å‚æ•°: {params}")
        print(f"   æ•°æ®è¡Œæ•°: {len(df)}")

        # å¦‚æœcrypto_ctaå¯ç”¨ï¼Œä½¿ç”¨çœŸå®å› å­
        if CTA_AVAILABLE:
            return calculate_real_factor_signals(df, strategy, params)
        else:
            print("âš ï¸ crypto_ctaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–å®ç°")
            return calculate_fallback_signals(df, strategy, params)

    except Exception as e:
        print(f"âŒ è®¡ç®—ç­–ç•¥ä¿¡å·å¤±è´¥: {e}")
        return None

def calculate_real_factor_signals(df: pd.DataFrame, strategy: str, params: dict) -> Optional[pd.DataFrame]:
    """ä½¿ç”¨çœŸå®çš„crypto_ctaå› å­è®¡ç®—ä¿¡å·"""
    try:
        # å‚æ•°æ ¼å¼è½¬æ¢ï¼šå°†APIå‚æ•°è½¬æ¢ä¸ºå› å­å‡½æ•°æœŸæœ›çš„paraæ ¼å¼
        para = convert_params_to_factor_format(strategy, params)
        print(f"   è½¬æ¢åçš„å› å­å‚æ•°: {para}")

        # åŠ¨æ€å¯¼å…¥å¯¹åº”çš„å› å­æ¨¡å—
        factor_module = import_factor_module(strategy)
        if factor_module is None:
            print(f"âŒ æ— æ³•å¯¼å…¥å› å­æ¨¡å—: {strategy}")
            return calculate_fallback_signals(df, strategy, params)

        # è°ƒç”¨å› å­çš„signalå‡½æ•°
        print(f"   è°ƒç”¨å› å­å‡½æ•°: {strategy}.signal()")
        df_result = factor_module.signal(df, para=para, proportion=1, leverage_rate=1)

        # éªŒè¯ç»“æœ
        if df_result is None or df_result.empty:
            print(f"âŒ å› å­è®¡ç®—è¿”å›ç©ºç»“æœ")
            return None

        # ç¡®ä¿posåˆ—å­˜åœ¨
        if 'pos' not in df_result.columns:
            if 'signal' in df_result.columns:
                # ä»signalè®¡ç®—pos
                df_result['pos'] = df_result['signal'].fillna(0)
            else:
                print("âš ï¸ å› å­ç»“æœä¸­ç¼ºå°‘poså’Œsignalåˆ—ï¼Œä½¿ç”¨é»˜è®¤æŒä»“")
                df_result['pos'] = 1

        print(f"âœ… å› å­è®¡ç®—æˆåŠŸï¼Œè¿”å› {len(df_result)} è¡Œæ•°æ®")
        return df_result

    except Exception as e:
        print(f"âŒ çœŸå®å› å­è®¡ç®—å¤±è´¥: {e}")
        return calculate_fallback_signals(df, strategy, params)

def import_factor_module(strategy: str):
    """åŠ¨æ€å¯¼å…¥å› å­æ¨¡å—"""
    try:
        # å› å­åç§°æ˜ å°„
        factor_map = {
            'sma': 'sma',
            'rsi': 'rsi',
            'macd': 'macd',
            'kdj': 'kdj',
            'atr_breakout': 'atr_breakout',
            'mean_reversion': 'mean_reversion',
            'xbx': 'xbx'
        }

        factor_name = factor_map.get(strategy.lower())
        if not factor_name:
            print(f"âŒ æœªçŸ¥çš„ç­–ç•¥ç±»å‹: {strategy}")
            return None

        # ä½¿ç”¨å†…ç½®çš„ç®€åŒ–å› å­å®ç°
        return get_builtin_factor(factor_name)

    except Exception as e:
        print(f"âŒ å¯¼å…¥å› å­æ¨¡å—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def get_builtin_factor(factor_name: str):
    """è·å–å†…ç½®çš„ç®€åŒ–å› å­å®ç°"""

    class BuiltinFactor:
        def __init__(self, name):
            self.name = name

        def signal(self, df, para=None, proportion=1, leverage_rate=1):
            """ç®€åŒ–çš„å› å­ä¿¡å·è®¡ç®—"""
            try:
                df = df.copy()

                if self.name == 'sma':
                    return self._sma_signal(df, para)
                elif self.name == 'rsi':
                    return self._rsi_signal(df, para)
                elif self.name == 'macd':
                    return self._macd_signal(df, para)
                elif self.name == 'kdj':
                    return self._kdj_signal(df, para)
                elif self.name == 'atr_breakout':
                    return self._atr_breakout_signal(df, para)
                elif self.name == 'mean_reversion':
                    return self._mean_reversion_signal(df, para)
                elif self.name == 'xbx':
                    return self._xbx_signal(df, para)
                else:
                    print(f"âš ï¸ æœªå®ç°çš„å› å­: {self.name}")
                    df['signal'] = 0
                    df['pos'] = 0
                    return df

            except Exception as e:
                print(f"âŒ å› å­ {self.name} è®¡ç®—å¤±è´¥: {e}")
                df['signal'] = 0
                df['pos'] = 0
                return df

        def _sma_signal(self, df, para):
            """SMAåŒå‡çº¿ç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬ï¼Œä½¿ç”¨çœŸæ­£çš„åŒå‡çº¿äº¤å‰"""
            # ä»å‚æ•°ä¸­è·å–çŸ­æœŸå’Œé•¿æœŸå‡çº¿å‘¨æœŸ
            short_window = para[0] if para and len(para) > 0 else 5
            long_window = para[1] if para and len(para) > 1 else 20

            # å¦‚æœåªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„åŒå‡çº¿è®¾ç½®
            if len(para) == 1:
                short_window = para[0]
                long_window = para[0] * 4  # é•¿æœŸå‡çº¿æ˜¯çŸ­æœŸçš„4å€

            # è®¡ç®—çœŸæ­£çš„åŒå‡çº¿
            df['ma_short'] = df['close'].rolling(window=short_window, min_periods=1).mean()
            df['ma_long'] = df['close'].rolling(window=long_window, min_periods=1).mean()

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = np.nan
            current_position = 0  # 0: ç©ºä»“, 1: å¤šä»“

            # è®¡ç®—äº¤å‰ä¿¡å· - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘å™ªéŸ³äº¤æ˜“
            for i in range(long_window, len(df)):  # ä»é•¿æœŸå‡çº¿æœ‰æ•ˆæ•°æ®å¼€å§‹
                ma_short_curr = df.iloc[i]['ma_short']
                ma_long_curr = df.iloc[i]['ma_long']
                ma_short_prev = df.iloc[i-1]['ma_short']
                ma_long_prev = df.iloc[i-1]['ma_long']

                if pd.notna(ma_short_curr) and pd.notna(ma_long_curr) and pd.notna(ma_short_prev) and pd.notna(ma_long_prev):
                    # æ·»åŠ è¿‡æ»¤æ¡ä»¶ï¼šåªæœ‰æ˜æ˜¾çš„äº¤å‰æ‰äº§ç”Ÿä¿¡å·
                    short_above_long = ma_short_curr > ma_long_curr
                    short_above_long_prev = ma_short_prev > ma_long_prev

                    # è®¡ç®—å‡çº¿å·®å€¼çš„ç™¾åˆ†æ¯”ï¼Œé¿å…å¾®å°æ³¢åŠ¨
                    ma_diff_pct = abs(ma_short_curr - ma_long_curr) / ma_long_curr

                    # åªæœ‰å½“å‡çº¿å·®å€¼è¶…è¿‡0.2%æ—¶æ‰è€ƒè™‘äº¤æ˜“ä¿¡å·ï¼ˆå‡å°‘å™ªéŸ³ï¼‰
                    if ma_diff_pct > 0.002:
                        # ä¸Šç©¿ï¼šä»ç©ºä»“åˆ°å¤šä»“
                        if short_above_long and not short_above_long_prev and current_position == 0:
                            df.iloc[i, df.columns.get_loc('signal')] = 1
                            current_position = 1
                        # ä¸‹ç©¿ï¼šä»å¤šä»“åˆ°ç©ºä»“
                        elif not short_above_long and short_above_long_prev and current_position == 1:
                            df.iloc[i, df.columns.get_loc('signal')] = 0
                            current_position = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['ma_short', 'ma_long'], axis=1, inplace=True, errors='ignore')
            return df

        def _rsi_signal(self, df, para):
            """RSIç­–ç•¥"""
            period = para[0] if para and len(para) > 0 else 14
            overbought = para[1] if para and len(para) > 1 else 70
            oversold = para[2] if para and len(para) > 2 else 30

            # è®¡ç®—RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / (loss + 1e-10)  # é¿å…é™¤é›¶
            df['rsi'] = 100 - (100 / (1 + rs))

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = 0

            # è®¡ç®—RSIä¿¡å· - ä¿®æ­£ç‰ˆæœ¬ï¼Œæ¶ˆé™¤å‰ç»åå·®
            current_signal = 0
            for i in range(len(df)):
                rsi_val = df.iloc[i]['rsi']
                if pd.notna(rsi_val):
                    # è¶…å–ä¹°å…¥
                    if rsi_val < oversold and current_signal == 0:
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_signal = 1
                    # è¶…ä¹°å–å‡º
                    elif rsi_val > overbought and current_signal == 1:
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_signal = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['rsi'], axis=1, inplace=True, errors='ignore')
            return df

        def _macd_signal(self, df, para):
            """MACDç­–ç•¥"""
            fast_period = para[0] if para and len(para) > 0 else 12
            slow_period = para[1] if para and len(para) > 1 else 26
            signal_period = para[2] if para and len(para) > 2 else 9

            # è®¡ç®—MACD
            ema_fast = df['close'].ewm(span=fast_period).mean()
            ema_slow = df['close'].ewm(span=slow_period).mean()
            df['macd'] = ema_fast - ema_slow
            df['macd_signal'] = df['macd'].ewm(span=signal_period).mean()
            df['macd_hist'] = df['macd'] - df['macd_signal']

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = 0
            df['pos'] = 0

            # è®¡ç®—MACDä¿¡å·
            current_pos = 0
            for i in range(1, len(df)):
                macd_curr = df.iloc[i]['macd']
                signal_curr = df.iloc[i]['macd_signal']
                macd_prev = df.iloc[i-1]['macd']
                signal_prev = df.iloc[i-1]['macd_signal']

                if pd.notna(macd_curr) and pd.notna(signal_curr):
                    # MACDä¸Šç©¿ä¿¡å·çº¿ï¼šä¹°å…¥
                    if macd_curr > signal_curr and macd_prev <= signal_prev:
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_pos = 1
                    # MACDä¸‹ç©¿ä¿¡å·çº¿ï¼šå–å‡º
                    elif macd_curr < signal_curr and macd_prev >= signal_prev:
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_pos = 0

                df.iloc[i, df.columns.get_loc('pos')] = current_pos

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['macd', 'macd_signal', 'macd_hist'], axis=1, inplace=True, errors='ignore')
            return df

        def _kdj_signal(self, df, para):
            """KDJç­–ç•¥"""
            period = para[0] if para and len(para) > 0 else 9
            k_period = para[1] if para and len(para) > 1 else 3
            d_period = para[2] if para and len(para) > 2 else 3

            # è®¡ç®—KDJ
            low_min = df['low'].rolling(window=period).min()
            high_max = df['high'].rolling(window=period).max()
            rsv = (df['close'] - low_min) / (high_max - low_min + 1e-10) * 100

            df['k'] = rsv.ewm(alpha=1/k_period).mean()
            df['d'] = df['k'].ewm(alpha=1/d_period).mean()
            df['j'] = 3 * df['k'] - 2 * df['d']

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = 0
            df['pos'] = 0

            # è®¡ç®—KDJä¿¡å·
            current_pos = 0
            for i in range(len(df)):
                k_val = df.iloc[i]['k']
                d_val = df.iloc[i]['d']

                if pd.notna(k_val) and pd.notna(d_val):
                    # Kçº¿ä¸Šç©¿Dçº¿ä¸”åœ¨è¶…å–åŒºï¼šä¹°å…¥
                    if k_val > d_val and k_val < 20 and current_pos == 0:
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_pos = 1
                    # Kçº¿ä¸‹ç©¿Dçº¿ä¸”åœ¨è¶…ä¹°åŒºï¼šå–å‡º
                    elif k_val < d_val and k_val > 80 and current_pos == 1:
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_pos = 0

                df.iloc[i, df.columns.get_loc('pos')] = current_pos

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['k', 'd', 'j'], axis=1, inplace=True, errors='ignore')
            return df

        def _atr_breakout_signal(self, df, para):
            """ATRçªç ´ç­–ç•¥"""
            period = para[0] if para and len(para) > 0 else 14
            entry_mult = para[1] if para and len(para) > 1 else 2.0
            exit_mult = para[2] if para and len(para) > 2 else 1.0

            # è®¡ç®—ATR
            df['tr'] = np.maximum(df['high'] - df['low'],
                                 np.maximum(abs(df['high'] - df['close'].shift(1)),
                                           abs(df['low'] - df['close'].shift(1))))
            df['atr'] = df['tr'].rolling(window=period).mean()

            # ä¿®æ­£ï¼šä½¿ç”¨å‰ä¸€æœŸçš„æ•°æ®è®¡ç®—çªç ´çº¿ï¼Œé¿å…å‰ç»åå·®
            df['high_max'] = df['high'].rolling(window=period).max().shift(1)
            df['low_min'] = df['low'].rolling(window=period).min().shift(1)
            df['atr_prev'] = df['atr'].shift(1)

            df['upper_band'] = df['high_max'] + entry_mult * df['atr_prev']
            df['lower_band'] = df['low_min'] - entry_mult * df['atr_prev']

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = 0

            # è®¡ç®—çªç ´ä¿¡å· - ä½¿ç”¨å½“æœŸæ”¶ç›˜ä»·ä¸å‰æœŸè®¡ç®—çš„çªç ´çº¿æ¯”è¾ƒ
            current_signal = 0
            for i in range(len(df)):
                close_price = df.iloc[i]['close']
                upper_band = df.iloc[i]['upper_band']
                lower_band = df.iloc[i]['lower_band']

                if pd.notna(close_price) and pd.notna(upper_band) and pd.notna(lower_band):
                    # å‘ä¸Šçªç ´ï¼šç”Ÿæˆä¹°å…¥ä¿¡å·
                    if close_price > upper_band and current_signal == 0:
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_signal = 1
                    # å‘ä¸‹çªç ´ï¼šç”Ÿæˆå–å‡ºä¿¡å·
                    elif close_price < lower_band and current_signal == 1:
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_signal = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['tr', 'atr', 'high_max', 'low_min', 'atr_prev', 'upper_band', 'lower_band'],
                   axis=1, inplace=True, errors='ignore')
            return df

        def _mean_reversion_signal(self, df, para):
            """å‡å€¼å›å½’ç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬ï¼Œæ¶ˆé™¤å‰ç»åå·®"""
            period = para[0] if para and len(para) > 0 else 20
            entry_thresh = para[1] if para and len(para) > 1 else 2.0
            exit_thresh = para[2] if para and len(para) > 2 else 0.5

            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            df['mean'] = df['close'].rolling(window=period).mean()
            df['std'] = df['close'].rolling(window=period).std()
            df['z_score'] = (df['close'] - df['mean']) / (df['std'] + 1e-10)

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = 0

            # è®¡ç®—å‡å€¼å›å½’ä¿¡å·
            current_signal = 0
            for i in range(len(df)):
                z_score = df.iloc[i]['z_score']

                if pd.notna(z_score):
                    # ä»·æ ¼åç¦»å‡å€¼è¿‡å¤šï¼šåå‘æ“ä½œ
                    if z_score < -entry_thresh and current_signal == 0:  # ä»·æ ¼è¿‡ä½ï¼Œä¹°å…¥
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_signal = 1
                    elif z_score > -exit_thresh and current_signal == 1:  # å›å½’å‡å€¼ï¼Œå–å‡º
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_signal = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['mean', 'std', 'z_score'], axis=1, inplace=True, errors='ignore')
            return df

        def _xbx_signal(self, df, para):
            """ä¿®æ­£å¸ƒæ—å¸¦ç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬ï¼Œæ¶ˆé™¤å‰ç»åå·®"""
            period = para[0] if para and len(para) > 0 else 20
            std_mult = para[1] if para and len(para) > 1 else 2.0

            # è®¡ç®—å¸ƒæ—å¸¦
            df['bb_middle'] = df['close'].rolling(window=period).mean()
            df['bb_std'] = df['close'].rolling(window=period).std()
            df['bb_upper'] = df['bb_middle'] + std_mult * df['bb_std']
            df['bb_lower'] = df['bb_middle'] - std_mult * df['bb_std']

            # åˆå§‹åŒ–ä¿¡å·åˆ—
            df['signal'] = 0

            # è®¡ç®—å¸ƒæ—å¸¦ä¿¡å·
            current_signal = 0
            for i in range(len(df)):
                close_price = df.iloc[i]['close']
                bb_upper = df.iloc[i]['bb_upper']
                bb_lower = df.iloc[i]['bb_lower']
                bb_middle = df.iloc[i]['bb_middle']

                if pd.notna(close_price) and pd.notna(bb_upper) and pd.notna(bb_lower):
                    # è§¦åŠä¸‹è½¨ï¼šä¹°å…¥
                    if close_price <= bb_lower and current_signal == 0:
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_signal = 1
                    # å›åˆ°ä¸­è½¨ï¼šå–å‡º
                    elif close_price >= bb_middle and current_signal == 1:
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_signal = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['bb_middle', 'bb_std', 'bb_upper', 'bb_lower'], axis=1, inplace=True, errors='ignore')
            return df

    return BuiltinFactor(factor_name)

def convert_params_to_factor_format(strategy: str, params: dict) -> list:
    """å°†APIå‚æ•°è½¬æ¢ä¸ºå› å­å‡½æ•°æœŸæœ›çš„paraæ ¼å¼"""
    try:
        strategy = strategy.lower()

        if strategy == 'sma':
            # SMAå› å­æœŸæœ›: [short_window, long_window]
            short_window = params.get('short_window', params.get('period', 5))
            long_window = params.get('long_window', short_window * 4)
            return [short_window, long_window]

        elif strategy == 'rsi':
            # RSIå› å­æœŸæœ›: [period, overbought_level, oversold_level]
            period = params.get('period', 14)
            overbought = params.get('overbought', 70)
            oversold = params.get('oversold', 30)
            return [period, overbought, oversold]

        elif strategy == 'macd':
            # MACDå› å­æœŸæœ›: [fast_period, slow_period, signal_period]
            fast = params.get('fast_period', 12)
            slow = params.get('slow_period', 26)
            signal = params.get('signal_period', 9)
            return [fast, slow, signal]

        elif strategy == 'kdj':
            # KDJå› å­æœŸæœ›: [period, k_period, d_period]
            period = params.get('period', 9)
            k_period = params.get('k_period', 3)
            d_period = params.get('d_period', 3)
            return [period, k_period, d_period]

        elif strategy == 'atr_breakout':
            # ATRçªç ´å› å­æœŸæœ›: [period, entry_multiplier, exit_multiplier]
            period = params.get('period', 14)
            entry_mult = params.get('entry_multiplier', 2.0)
            exit_mult = params.get('exit_multiplier', 1.0)
            return [period, entry_mult, exit_mult]

        elif strategy == 'mean_reversion':
            # å‡å€¼å›å½’å› å­æœŸæœ›: [period, entry_threshold, exit_threshold]
            period = params.get('period', 20)
            entry_thresh = params.get('entry_threshold', 2.0)
            exit_thresh = params.get('exit_threshold', 0.5)
            return [period, entry_thresh, exit_thresh]

        elif strategy == 'xbx':
            # XBXå› å­æœŸæœ›: [period, std_multiplier]
            period = params.get('period', 20)
            std_mult = params.get('std_multiplier', 2.0)
            return [period, std_mult]

        else:
            print(f"âš ï¸ æœªçŸ¥ç­–ç•¥ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°: {strategy}")
            return [20, 2.0]  # é»˜è®¤å‚æ•°

    except Exception as e:
        print(f"âŒ å‚æ•°è½¬æ¢å¤±è´¥: {e}")
        return [20, 2.0]  # é»˜è®¤å‚æ•°

def ensure_position_lag(df: pd.DataFrame) -> pd.DataFrame:
    """ç¡®ä¿æŒä»“ä¿¡å·æ­£ç¡®å»¶è¿Ÿï¼Œé¿å…å‰ç»åå·®"""
    try:
        df = df.copy()

        # å¦‚æœå­˜åœ¨signalåˆ—ä½†posåˆ—ä¸æ­£ç¡®ï¼Œé‡æ–°è®¡ç®—pos
        if 'signal' in df.columns:
            # æ¸…ç†signalåˆ—ï¼Œå‘å‰å¡«å……NaNå€¼
            df['signal_clean'] = df['signal'].ffill().fillna(0)

            # å…³é”®ï¼šä½¿ç”¨shift(1)ç¡®ä¿æŒä»“åœ¨ä¸‹ä¸€æœŸç”Ÿæ•ˆï¼Œé¿å…å‰ç»åå·®
            df['pos'] = df['signal_clean'].shift(1)
            df['pos'] = df['pos'].fillna(0)

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['signal_clean'], axis=1, inplace=True, errors='ignore')

        # å¦‚æœæ²¡æœ‰signalåˆ—ä½†æœ‰posåˆ—ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é¢å¤–å»¶è¿Ÿ
        elif 'pos' in df.columns:
            # æ£€æŸ¥posæ˜¯å¦å·²ç»æ­£ç¡®å»¶è¿Ÿï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦å­˜åœ¨å³æ—¶å“åº”ï¼‰
            # è¿™é‡Œæˆ‘ä»¬ä¿æŒç°æœ‰çš„posï¼Œå‡è®¾å®ƒå·²ç»æ­£ç¡®å¤„ç†
            pass
        else:
            # å¦‚æœæ—¢æ²¡æœ‰signalä¹Ÿæ²¡æœ‰posï¼Œåˆ›å»ºé»˜è®¤çš„ç©ºä»“ä½
            df['pos'] = 0

        return df

    except Exception as e:
        print(f"âŒ æŒä»“å»¶è¿Ÿå¤„ç†å¤±è´¥: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè‡³å°‘ç¡®ä¿æœ‰posåˆ—
        if 'pos' not in df.columns:
            df['pos'] = 0
        return df

def calculate_fallback_signals(df: pd.DataFrame, strategy: str, params: dict) -> Optional[pd.DataFrame]:
    """ç®€åŒ–çš„ç­–ç•¥ä¿¡å·è®¡ç®—ï¼ˆå½“crypto_ctaä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
    try:
        df = df.copy()

        if strategy == 'sma':
            # ç®€å•ç§»åŠ¨å¹³å‡ç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬ï¼Œæ¶ˆé™¤å‰ç»åå·®
            short_window = params.get('short_window', params.get('period', 5))
            long_window = params.get('long_window', short_window * 2)

            df['sma_short'] = df['close'].rolling(window=short_window).mean()
            df['sma_long'] = df['close'].rolling(window=long_window).mean()

            # ç”Ÿæˆäº¤æ˜“ä¿¡å·ï¼ˆä¸æ˜¯æŒä»“ä¿¡å·ï¼‰
            df['signal'] = 0

            # ä¿®æ­£ï¼šä»ç¬¬1è¡Œå¼€å§‹ï¼ˆéœ€è¦å‰ä¸€æœŸæ•°æ®è¿›è¡Œæ¯”è¾ƒï¼‰
            for i in range(1, len(df)):
                ma_short_curr = df.iloc[i]['sma_short']
                ma_long_curr = df.iloc[i]['sma_long']
                ma_short_prev = df.iloc[i-1]['sma_short']
                ma_long_prev = df.iloc[i-1]['sma_long']

                if pd.notna(ma_short_curr) and pd.notna(ma_long_curr) and pd.notna(ma_short_prev) and pd.notna(ma_long_prev):
                    # ä¸Šç©¿ï¼šç”Ÿæˆä¹°å…¥ä¿¡å·
                    if ma_short_curr > ma_long_curr and ma_short_prev <= ma_long_prev:
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                    # ä¸‹ç©¿ï¼šç”Ÿæˆå–å‡ºä¿¡å·
                    elif ma_short_curr < ma_long_curr and ma_short_prev >= ma_long_prev:
                        df.iloc[i, df.columns.get_loc('signal')] = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['sma_short', 'sma_long'], axis=1, inplace=True, errors='ignore')

        elif strategy == 'rsi':
            # RSIç­–ç•¥ - ä¿®æ­£ç‰ˆæœ¬ï¼Œæ¶ˆé™¤å‰ç»åå·®
            period = params.get('period', 14)
            overbought = params.get('overbought', 70)
            oversold = params.get('oversold', 30)

            # è®¡ç®—RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))

            # ç”Ÿæˆäº¤æ˜“ä¿¡å·ï¼ˆä¸æ˜¯æŒä»“ä¿¡å·ï¼‰
            df['signal'] = np.nan
            current_signal = 0

            for i in range(len(df)):
                rsi_val = df.iloc[i]['rsi']
                if pd.notna(rsi_val):
                    if rsi_val < oversold and current_signal == 0:  # è¶…å–æ—¶ä¹°å…¥
                        df.iloc[i, df.columns.get_loc('signal')] = 1
                        current_signal = 1
                    elif rsi_val > overbought and current_signal == 1:  # è¶…ä¹°æ—¶å–å‡º
                        df.iloc[i, df.columns.get_loc('signal')] = 0
                        current_signal = 0

            # æ¸…ç†ä¸´æ—¶åˆ—
            df.drop(['rsi'], axis=1, inplace=True, errors='ignore')

        else:
            # é»˜è®¤ä¹°å…¥æŒæœ‰ç­–ç•¥
            print(f"âš ï¸ ç­–ç•¥ {strategy} æš‚ä¸æ”¯æŒç®€åŒ–å®ç°ï¼Œä½¿ç”¨ä¹°å…¥æŒæœ‰ç­–ç•¥")
            df['signal'] = 1  # ç”Ÿæˆä¿¡å·è€Œä¸æ˜¯ç›´æ¥è®¾ç½®æŒä»“

        # å…³é”®æ­¥éª¤ï¼šå°†ä¿¡å·è½¬æ¢ä¸ºæŒä»“ï¼Œå¹¶åº”ç”¨æ­£ç¡®çš„å»¶è¿Ÿ
        # å‘å‰å¡«å……ä¿¡å·
        df['signal'] = df['signal'].ffill().fillna(0)

        # åº”ç”¨æŒä»“å»¶è¿Ÿï¼ˆä¿¡å·åœ¨ä¸‹ä¸€æœŸç”Ÿæ•ˆï¼‰
        df['pos'] = df['signal'].shift(1).fillna(0)

        return df

    except Exception as e:
        print(f"âŒ ç®€åŒ–ç­–ç•¥è®¡ç®—å¤±è´¥: {e}")
        return None

def calculate_equity_curve_simple(df: pd.DataFrame, leverage_rate: float, c_rate: float, slippage: float) -> pd.DataFrame:
    """ä¿®æ­£çš„èµ„é‡‘æ›²çº¿è®¡ç®— - æ¶ˆé™¤å‰ç»åå·®å’Œä¿®æ­£äº¤æ˜“æˆæœ¬è®¡ç®—"""
    try:
        df = df.copy()

        # ç¡®ä¿æŒä»“ä¿¡å·å·²ç»æ­£ç¡®å»¶è¿Ÿï¼ˆé¿å…å‰ç»åå·®ï¼‰
        df = ensure_position_lag(df)

        # è®¡ç®—æŒä»“å˜åŒ–
        df['pos_change'] = df['pos'].diff().fillna(0)

        # è®¡ç®—æ”¶ç›Šç‡
        df['price_change'] = df['close'].pct_change().fillna(0)

        # è®¡ç®—ç­–ç•¥æ”¶ç›Šç‡ï¼ˆä½¿ç”¨å‰ä¸€æœŸæŒä»“ï¼Œé¿å…å‰ç»åå·®ï¼‰
        df['strategy_return'] = df['pos'].shift(1) * df['price_change'] * leverage_rate

        # ä¿®æ­£ï¼šåŸºäºäº¤æ˜“ä»·å€¼è®¡ç®—äº¤æ˜“æˆæœ¬ï¼Œä½†ä½¿ç”¨æ›´åˆç†çš„æˆæœ¬æ¨¡å‹
        # è®¡ç®—äº¤æ˜“ä»·å€¼ï¼ˆç»å¯¹æŒä»“å˜åŒ– * å½“æœŸæ”¶ç›˜ä»·ï¼‰
        df['trade_value'] = abs(df['pos_change']) * df['close']

        # åˆå§‹åŒ–èµ„é‡‘æ›²çº¿ç”¨äºè®¡ç®—äº¤æ˜“æˆæœ¬ç™¾åˆ†æ¯”
        initial_capital = 10000.0  # åˆå§‹èµ„é‡‘
        df['equity_curve_temp'] = float(initial_capital)
        df['net_return'] = 0.0  # åˆå§‹åŒ–å‡€æ”¶ç›Šç‡åˆ—
        df['trade_cost'] = 0.0  # åˆå§‹åŒ–äº¤æ˜“æˆæœ¬åˆ—

        # é€è¡Œè®¡ç®—èµ„é‡‘æ›²çº¿å’Œäº¤æ˜“æˆæœ¬
        for i in range(1, len(df)):
            # å‰ä¸€æœŸçš„èµ„é‡‘
            prev_equity = df.iloc[i-1]['equity_curve_temp']

            # å½“æœŸç­–ç•¥æ”¶ç›Šï¼ˆåŸºäºå‰ä¸€æœŸèµ„é‡‘ï¼‰
            strategy_pnl = df.iloc[i]['strategy_return'] * prev_equity

            # ä¿®æ­£çš„äº¤æ˜“æˆæœ¬è®¡ç®—ï¼šåªå¯¹å®é™…äº¤æ˜“æ”¶å–æˆæœ¬ï¼Œä¸”åŸºäºèµ„é‡‘è€Œéäº¤æ˜“ä»·å€¼
            if df.iloc[i]['pos_change'] != 0:
                # äº¤æ˜“æˆæœ¬åŸºäºå½“æœŸèµ„é‡‘çš„ç™¾åˆ†æ¯”ï¼Œè€Œä¸æ˜¯äº¤æ˜“ä»·å€¼
                trade_cost_absolute = prev_equity * abs(df.iloc[i]['pos_change']) * (c_rate + slippage)
            else:
                trade_cost_absolute = 0

            # å­˜å‚¨äº¤æ˜“æˆæœ¬
            df.iloc[i, df.columns.get_loc('trade_cost')] = trade_cost_absolute

            # å‡€æ”¶ç›Š
            net_pnl = strategy_pnl - trade_cost_absolute

            # æ›´æ–°èµ„é‡‘æ›²çº¿
            df.iloc[i, df.columns.get_loc('equity_curve_temp')] = prev_equity + net_pnl

            # è®¡ç®—å‡€æ”¶ç›Šç‡ï¼ˆç›¸å¯¹äºå‰ä¸€æœŸèµ„é‡‘ï¼‰
            df.iloc[i, df.columns.get_loc('net_return')] = net_pnl / prev_equity if prev_equity > 0 else 0

        # æ ‡å‡†åŒ–èµ„é‡‘æ›²çº¿ï¼ˆä»¥åˆå§‹èµ„é‡‘ä¸º1ï¼‰
        df['equity_curve'] = df['equity_curve_temp'] / initial_capital

        # è®¡ç®—å›æ’¤
        df['peak'] = df['equity_curve'].expanding().max()
        df['drawdown'] = (df['equity_curve'] - df['peak']) / df['peak']

        # æ¸…ç†ä¸´æ—¶åˆ—
        df.drop(['equity_curve_temp', 'trade_value'], axis=1, inplace=True, errors='ignore')

        return df

    except Exception as e:
        print(f"Error calculating equity curve: {e}")
        return df

def calculate_backtest_statistics(df: pd.DataFrame, symbol: str, request: BacktestRequest) -> BacktestResult:
    """ä¿®æ­£çš„å›æµ‹ç»Ÿè®¡æŒ‡æ ‡è®¡ç®— - ä½¿ç”¨ä¸€è‡´çš„å¹´åŒ–æ–¹æ³•å’Œæ­£ç¡®çš„é£é™©æŒ‡æ ‡"""
    try:
        import numpy as np
        from datetime import timedelta

        # åŸºæœ¬ç»Ÿè®¡
        final_equity = df['equity_curve'].iloc[-1]
        final_return = final_equity - 1

        # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡ - ä¿®æ­£ï¼šä½¿ç”¨ä¸€è‡´çš„äº¤æ˜“æ—¥è®¡ç®—
        start_date = df['candle_begin_time'].iloc[0]
        end_date = df['candle_begin_time'].iloc[-1]
        days = (end_date - start_date).days

        # æ ¹æ®æ•°æ®é¢‘ç‡ç¡®å®šå¹´åŒ–å› å­
        if request.rule_type == '1H':
            periods_per_year = 365 * 24  # æ¯å¹´å°æ—¶æ•°
            periods_in_data = len(df)
        elif request.rule_type == '4H':
            periods_per_year = 365 * 6   # æ¯å¹´4å°æ—¶å‘¨æœŸæ•°
            periods_in_data = len(df)
        elif request.rule_type == '1D':
            periods_per_year = 365       # æ¯å¹´å¤©æ•°
            periods_in_data = len(df)
        else:
            # é»˜è®¤ä½¿ç”¨å¤©æ•°
            periods_per_year = 365
            periods_in_data = max(days, 1)

        # ä¿®æ­£çš„å¹´åŒ–æ”¶ç›Šç‡è®¡ç®—
        if periods_in_data > 0:
            annual_return = (final_equity ** (periods_per_year / periods_in_data)) - 1
        else:
            annual_return = 0

        # è®¡ç®—æœ€å¤§å›æ’¤
        df['peak'] = df['equity_curve'].expanding().max()
        df['drawdown'] = (df['equity_curve'] - df['peak']) / df['peak']
        max_drawdown = abs(df['drawdown'].min())

        # ä¿®æ­£çš„é£é™©æŒ‡æ ‡è®¡ç®—
        returns = df['net_return'].dropna()
        if len(returns) > 1:
            # ä½¿ç”¨ä¸€è‡´çš„å¹´åŒ–å› å­
            if request.rule_type == '1H':
                annualization_factor = np.sqrt(365 * 24)
            elif request.rule_type == '4H':
                annualization_factor = np.sqrt(365 * 6)
            elif request.rule_type == '1D':
                annualization_factor = np.sqrt(365)
            else:
                annualization_factor = np.sqrt(252)  # é»˜è®¤äº¤æ˜“æ—¥

            # å¹´åŒ–æ³¢åŠ¨ç‡
            volatility = returns.std() * annualization_factor

            # ä¿®æ­£çš„å¤æ™®æ¯”ç‡ï¼ˆå‡è®¾æ— é£é™©åˆ©ç‡ä¸º0ï¼Œé€‚ç”¨äºåŠ å¯†è´§å¸ï¼‰
            sharpe_ratio = annual_return / volatility if volatility > 0 else 0
        else:
            volatility = 0
            sharpe_ratio = 0

        # è®¡ç®—äº¤æ˜“ç»Ÿè®¡
        trades = df[df['pos_change'] != 0].copy()
        total_trades = len(trades)

        if total_trades > 0:
            # ç®€åŒ–çš„äº¤æ˜“åˆ†æ
            winning_trades = len(trades[trades['net_return'] > 0])
            losing_trades = total_trades - winning_trades
            win_rate = winning_trades / total_trades

            avg_win = trades[trades['net_return'] > 0]['net_return'].mean() if winning_trades > 0 else 0
            avg_loss = trades[trades['net_return'] < 0]['net_return'].mean() if losing_trades > 0 else 0

            total_profit = trades[trades['net_return'] > 0]['net_return'].sum()
            total_loss = abs(trades[trades['net_return'] < 0]['net_return'].sum())
            profit_factor = total_profit / total_loss if total_loss > 0 else 0
        else:
            winning_trades = 0
            losing_trades = 0
            win_rate = 0
            avg_win = 0
            avg_loss = 0
            profit_factor = 0

        # ç”Ÿæˆèµ„é‡‘æ›²çº¿æ•°æ®
        equity_curve = []
        for _, row in df.iterrows():
            equity_curve.append({
                "date": row['candle_begin_time'].strftime("%Y-%m-%d %H:%M:%S"),
                "value": float(row['equity_curve'] * 10000),  # å‡è®¾åˆå§‹èµ„é‡‘10000
                "drawdown": float(abs(row['drawdown']))
            })

        # ç”Ÿæˆäº¤æ˜“è®°å½•
        trade_records = []
        for i, (_, row) in enumerate(trades.iterrows()):
            if row['pos_change'] != 0:
                trade_records.append(TradeRecord(
                    id=f"trade_{symbol}_{i:06d}",
                    symbol=symbol,
                    side="buy" if row['pos_change'] > 0 else "sell",
                    quantity=abs(row['pos_change']),
                    price=float(row['close']),
                    timestamp=row['candle_begin_time'],
                    pnl=float(row['net_return'] * 10000),
                    commission=float(row['trade_cost'] * 10000 * 0.5),
                    slippage=float(row['trade_cost'] * 10000 * 0.5)
                ))

        # ç”Ÿæˆæœˆåº¦æ”¶ç›Š
        monthly_returns = []
        df_monthly = df.set_index('candle_begin_time')
        monthly_data = df_monthly['net_return'].resample('ME').sum()

        for date, return_val in monthly_data.items():
            monthly_returns.append(MonthlyReturn(
                year=date.year,
                month=date.month,
                return_value=float(return_val)
            ))

        # è®¡ç®—å…¶ä»–é£é™©æŒ‡æ ‡
        sortino_ratio = 0
        calmar_ratio = annual_return / max_drawdown if max_drawdown > 0 else 0
        var_95 = np.percentile(returns, 5) if len(returns) > 0 else 0
        cvar_95 = returns[returns <= var_95].mean() if len(returns) > 0 else 0

        result = BacktestResult(
            task_id="",  # å°†åœ¨è°ƒç”¨å¤„è®¾ç½®
            symbol=symbol,
            strategy=request.strategy,
            parameters=request.parameters,
            final_return=final_return,
            annual_return=annual_return,
            max_drawdown=max_drawdown,
            sharpe_ratio=sharpe_ratio,
            sortino_ratio=sortino_ratio,
            calmar_ratio=calmar_ratio,
            win_rate=win_rate,
            profit_factor=profit_factor,
            total_trades=total_trades,
            winning_trades=winning_trades,
            losing_trades=losing_trades,
            avg_win=avg_win,
            avg_loss=avg_loss,
            max_consecutive_wins=0,  # éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
            max_consecutive_losses=0,  # éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
            volatility=volatility,
            skewness=float(returns.skew()) if len(returns) > 2 else 0,
            kurtosis=float(returns.kurtosis()) if len(returns) > 2 else 0,
            var_95=var_95,
            cvar_95=cvar_95,
            equity_curve=equity_curve,
            drawdown_periods=[],  # éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
            monthly_returns=monthly_returns,
            trade_records=trade_records,
            created_at=datetime.now()
        )

        return result

    except Exception as e:
        print(f"Error calculating backtest statistics: {e}")
        # è¿”å›é»˜è®¤ç»“æœ
        return BacktestResult(
            task_id="",
            symbol=symbol,
            strategy=request.strategy,
            parameters=request.parameters,
            final_return=0,
            annual_return=0,
            max_drawdown=0,
            sharpe_ratio=0,
            win_rate=0,
            profit_factor=0,
            total_trades=0,
            equity_curve=[],
            trade_records=[],
            monthly_returns=[],
            created_at=datetime.now()
        )

@router.post("/run")
async def start_backtest(
    request: BacktestRequest,
    background_tasks: BackgroundTasks
):
    """å¯åŠ¨å›æµ‹ä»»åŠ¡ - ä»…ä½¿ç”¨çœŸå®æ•°æ®"""
    import logging

    # è®¾ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    task_id = str(uuid.uuid4())

    # è®°å½•å›æµ‹è¯·æ±‚
    logger.info(f"ğŸš€ å¯åŠ¨æ–°å›æµ‹ä»»åŠ¡: {task_id}")
    logger.info(f"   äº¤æ˜“å¯¹: {request.symbols}")
    logger.info(f"   ç­–ç•¥: {request.strategy}")
    logger.info(f"   æ—¶é—´èŒƒå›´: {request.date_start} åˆ° {request.date_end}")
    logger.info(f"   æ•°æ®å‘¨æœŸ: {request.rule_type}")
    logger.info(f"   æ•°æ®æ¥æº: ä»…ä½¿ç”¨BinanceçœŸå®æ•°æ®")

    # åˆ›å»ºä»»åŠ¡çŠ¶æ€
    task_status = BacktestStatus(
        task_id=task_id,
        status="pending",
        message="å›æµ‹ä»»åŠ¡å·²åˆ›å»º - å°†ä»…ä½¿ç”¨çœŸå®æ•°æ®",
        symbols_total=len(request.symbols)
    )
    backtest_tasks[task_id] = task_status

    # åœ¨åå°è¿è¡Œå›æµ‹
    background_tasks.add_task(run_backtest_task, task_id, request)

    return {
        "task_id": task_id,
        "message": "å›æµ‹å·²å¯åŠ¨ - ä»…ä½¿ç”¨BinanceçœŸå®æ•°æ®",
        "status": "pending",
        "data_source": "Binance API (çœŸå®æ•°æ®)"
    }

async def run_backtest_task(task_id: str, request: BacktestRequest):
    """åå°è¿è¡Œå›æµ‹ä»»åŠ¡ - ä»…ä½¿ç”¨çœŸå®æ•°æ®"""
    task_status = backtest_tasks[task_id]

    try:
        task_status.status = "running"
        task_status.message = "Starting backtest with real data only..."

        results = []

        for i, symbol in enumerate(request.symbols):
            task_status.message = f"Processing {symbol} with real data..."
            task_status.progress = (i / len(request.symbols)) * 100

            try:
                # å¼ºåˆ¶ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œå›æµ‹
                real_result = await run_real_backtest(symbol, request)
                if real_result:
                    real_result.task_id = task_id
                    results.append(real_result)
                    print(f"âœ… {symbol}: ä½¿ç”¨çœŸå®æ•°æ®å®Œæˆå›æµ‹")
                else:
                    # å¦‚æœæ— æ³•è·å–çœŸå®æ•°æ®ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡
                    error_msg = f"âŒ {symbol}: æ— æ³•è·å–çœŸå®æ•°æ®ï¼Œè·³è¿‡æ­¤äº¤æ˜“å¯¹"
                    print(error_msg)
                    task_status.message = f"Warning: {error_msg}"
                    continue

            except Exception as e:
                error_msg = f"âŒ {symbol}: å›æµ‹å¤±è´¥ - {str(e)}"
                print(error_msg)
                task_status.message = f"Error: {error_msg}"
                continue

            task_status.symbols_completed = i + 1

        if not results:
            task_status.status = "failed"
            task_status.message = "æ‰€æœ‰äº¤æ˜“å¯¹éƒ½æ— æ³•è·å–çœŸå®æ•°æ®ï¼Œå›æµ‹å¤±è´¥"
        else:
            task_status.status = "completed"
            task_status.progress = 100.0
            task_status.message = f"å›æµ‹å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} ä¸ªäº¤æ˜“å¯¹ï¼ˆä»…ä½¿ç”¨çœŸå®æ•°æ®ï¼‰"
            task_status.results = results

    except Exception as e:
        task_status.status = "failed"
        task_status.message = f"Backtest failed: {str(e)}"

@router.get("/status/{task_id}", response_model=BacktestStatus)
async def get_backtest_status(task_id: str):
    """è·å–å›æµ‹ä»»åŠ¡çŠ¶æ€"""
    if task_id not in backtest_tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    return backtest_tasks[task_id]


@router.get("/data-source-verification")
async def verify_data_sources():
    """éªŒè¯æ•°æ®æºé…ç½®å’Œå¯ç”¨æ€§"""
    import logging
    import os
    import aiohttp

    logger = logging.getLogger(__name__)
    verification_result = {
        "timestamp": datetime.now().isoformat(),
        "bn_data_config": {},
        "binance_api_status": {},
        "data_integrity": {},
        "recommendations": []
    }

    try:
        # 1. æ£€æŸ¥bn_dataé…ç½®
        bn_data_config_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'bn_data', 'config.py')
        if os.path.exists(bn_data_config_path):
            try:
                # è¯»å–bn_dataé…ç½®
                with open(bn_data_config_path, 'r', encoding='utf-8') as f:
                    config_content = f.read()

                # æ£€æŸ¥å…³é”®é…ç½®
                update_to_now = 'update_to_now = True' in config_content
                trade_type = 'swap' if "trade_type = 'swap'" in config_content else 'spot'

                verification_result["bn_data_config"] = {
                    "config_file_exists": True,
                    "update_to_now": update_to_now,
                    "trade_type": trade_type,
                    "status": "âœ… é…ç½®æ­£ç¡®" if update_to_now else "âŒ éœ€è¦è®¾ç½® update_to_now = True"
                }

                if not update_to_now:
                    verification_result["recommendations"].append(
                        "è®¾ç½® bn_data/config.py ä¸­çš„ update_to_now = True ä»¥è·å–2025å¹´æœ€æ–°æ•°æ®"
                    )

            except Exception as e:
                verification_result["bn_data_config"] = {
                    "config_file_exists": True,
                    "error": f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}",
                    "status": "âŒ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥"
                }
        else:
            verification_result["bn_data_config"] = {
                "config_file_exists": False,
                "status": "âŒ bn_dataé…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
            }
            verification_result["recommendations"].append("ç¡®ä¿bn_dataæ¨¡å—æ­£ç¡®å®‰è£…å’Œé…ç½®")

        # 2. æµ‹è¯•Binance APIè¿æ¥
        api_endpoints = [
            'https://fapi.binance.com/fapi/v1/ping',
            'https://api.binance.com/api/v3/ping'
        ]

        for endpoint in api_endpoints:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(endpoint, timeout=10) as response:
                        if response.status == 200:
                            verification_result["binance_api_status"][endpoint] = {
                                "status": "âœ… è¿æ¥æ­£å¸¸",
                                "response_time": "< 10s"
                            }
                        else:
                            verification_result["binance_api_status"][endpoint] = {
                                "status": f"âŒ è¿æ¥å¤±è´¥ ({response.status})",
                                "response_time": "N/A"
                            }
            except Exception as e:
                verification_result["binance_api_status"][endpoint] = {
                    "status": f"âŒ è¿æ¥å¼‚å¸¸: {e}",
                    "response_time": "N/A"
                }

        # 3. æ£€æŸ¥ç°æœ‰æ•°æ®å®Œæ•´æ€§
        data_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'bn_data', 'output', 'pickle_data')
        if os.path.exists(data_path):
            intervals = ['1H', '4H', '1D']
            symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']

            for interval in intervals:
                interval_path = os.path.join(data_path, interval)
                if os.path.exists(interval_path):
                    available_files = [f for f in os.listdir(interval_path) if f.endswith('.pkl')]
                    verification_result["data_integrity"][interval] = {
                        "available_symbols": len(available_files),
                        "expected_symbols": len(symbols),
                        "files": available_files,
                        "status": "âœ… æ•°æ®å®Œæ•´" if len(available_files) >= len(symbols) else "âš ï¸ æ•°æ®ä¸å®Œæ•´"
                    }
                else:
                    verification_result["data_integrity"][interval] = {
                        "available_symbols": 0,
                        "expected_symbols": len(symbols),
                        "files": [],
                        "status": "âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨"
                    }
        else:
            verification_result["data_integrity"] = {
                "status": "âŒ bn_dataè¾“å‡ºç›®å½•ä¸å­˜åœ¨"
            }
            verification_result["recommendations"].append("è¿è¡Œ 'cd bn_data && python main.py' ç”Ÿæˆæ•°æ®æ–‡ä»¶")

        # 4. ç”Ÿæˆæ€»ä½“å»ºè®®
        if not verification_result["recommendations"]:
            verification_result["recommendations"].append("âœ… æ‰€æœ‰é…ç½®æ­£ç¡®ï¼Œå¯ä»¥ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œå›æµ‹")

        verification_result["overall_status"] = "âœ… éªŒè¯é€šè¿‡" if not any(
            "âŒ" in str(v) for v in verification_result.values()
        ) else "âš ï¸ éœ€è¦ä¿®å¤é…ç½®é—®é¢˜"

        return verification_result

    except Exception as e:
        logger.error(f"æ•°æ®æºéªŒè¯å¤±è´¥: {e}")
        return {
            "timestamp": datetime.now().isoformat(),
            "error": f"éªŒè¯è¿‡ç¨‹å¤±è´¥: {e}",
            "overall_status": "âŒ éªŒè¯å¤±è´¥"
        }

@router.get("/results/{task_id}")
async def get_backtest_results(task_id: str):
    """è·å–å›æµ‹ç»“æœ"""
    if task_id not in backtest_tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    task_status = backtest_tasks[task_id]

    if task_status.status != "completed":
        raise HTTPException(status_code=400, detail="Backtest not completed yet")

    return {
        "task_id": task_id,
        "status": task_status.status,
        "results": task_status.results,
        "summary": {
            "total_symbols": len(task_status.results),
            "avg_return": sum(r.final_return for r in task_status.results) / len(task_status.results) if task_status.results else 0,
            "best_symbol": max(task_status.results, key=lambda x: x.final_return).symbol if task_status.results else None
        }
    }

@router.get("/tasks")
async def list_backtest_tasks():
    """è·å–æ‰€æœ‰å›æµ‹ä»»åŠ¡åˆ—è¡¨"""
    return [
        {
            "task_id": task_id,
            "status": task.status,
            "symbols_total": task.symbols_total,
            "symbols_completed": task.symbols_completed,
            "progress": task.progress,
            "message": task.message,
            "results": task.results or []
        }
        for task_id, task in backtest_tasks.items()
    ]

@router.post("/optimize")
async def start_optimization(
    request: OptimizationRequest,
    background_tasks: BackgroundTasks
):
    """å¯åŠ¨å‚æ•°ä¼˜åŒ–ä»»åŠ¡"""
    task_id = str(uuid.uuid4())

    # åˆ›å»ºä¼˜åŒ–ä»»åŠ¡
    task_status = BacktestStatus(
        task_id=task_id,
        status="pending",
        message="Parameter optimization task created",
        symbols_total=len(request.symbols)
    )
    backtest_tasks[task_id] = task_status

    # åœ¨åå°è¿è¡Œä¼˜åŒ–
    background_tasks.add_task(run_optimization_task, task_id, request)

    return {
        "task_id": task_id,
        "message": "Parameter optimization started",
        "status": "pending"
    }

async def run_optimization_task(task_id: str, request: OptimizationRequest):
    """åå°è¿è¡Œå‚æ•°ä¼˜åŒ–ä»»åŠ¡"""
    task_status = backtest_tasks[task_id]

    try:
        task_status.status = "running"
        task_status.message = "Running parameter optimization..."

        # å®ç°ç½‘æ ¼æœç´¢å‚æ•°ä¼˜åŒ–
        results = await run_grid_search_optimization(task_id, request)

        # æŒ‰ç…§å¤æ™®æ¯”ç‡æ’åºç»“æœ
        results.sort(key=lambda x: x.sharpe_ratio, reverse=True)

        task_status.results = results
        task_status.status = "completed"
        task_status.progress = 100.0
        task_status.message = f"Parameter optimization completed. Found {len(results)} parameter combinations."

    except Exception as e:
        task_status.status = "failed"
        task_status.message = f"Optimization failed: {str(e)}"

async def run_grid_search_optimization(task_id: str, request: OptimizationRequest) -> List[BacktestResult]:
    """è¿è¡Œç½‘æ ¼æœç´¢å‚æ•°ä¼˜åŒ–"""
    import itertools

    # ç”Ÿæˆå‚æ•°ç»„åˆ
    param_combinations = generate_parameter_combinations(request.parameter_ranges)

    print(f"ğŸ” å¼€å§‹å‚æ•°ä¼˜åŒ–ï¼Œå…± {len(param_combinations)} ä¸ªå‚æ•°ç»„åˆ")

    results = []
    task_status = backtest_tasks[task_id]

    for i, params in enumerate(param_combinations):
        try:
            # æ›´æ–°è¿›åº¦
            progress = (i / len(param_combinations)) * 100
            task_status.progress = progress
            task_status.message = f"Testing parameters {i+1}/{len(param_combinations)}: {params}"

            print(f"ğŸ“Š æµ‹è¯•å‚æ•°ç»„åˆ {i+1}/{len(param_combinations)}: {params}")

            # ä¸ºæ¯ä¸ªäº¤æ˜“å¯¹è¿è¡Œå›æµ‹
            for symbol in request.symbols:
                # åˆ›å»ºå›æµ‹è¯·æ±‚
                backtest_request = BacktestRequest(
                    symbols=[symbol],
                    strategy=request.strategy,
                    parameters=params,
                    date_start=request.date_start,
                    date_end=request.date_end,
                    rule_type=request.rule_type
                )

                # è¿è¡Œå›æµ‹
                result = await run_real_backtest(symbol, backtest_request)
                if result:
                    result.task_id = task_id
                    result.parameters = params  # ç¡®ä¿å‚æ•°è¢«æ­£ç¡®è®¾ç½®
                    results.append(result)
                    print(f"âœ… {symbol}: å‚æ•° {params} - å¤æ™®æ¯”ç‡: {result.sharpe_ratio:.3f}, æ”¶ç›Šç‡: {result.final_return:.3f}")
                else:
                    print(f"âŒ {symbol}: å‚æ•° {params} - å›æµ‹å¤±è´¥")

        except Exception as e:
            print(f"âŒ å‚æ•°ç»„åˆ {params} æµ‹è¯•å¤±è´¥: {e}")
            continue

    print(f"ğŸ¯ å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")
    return results

def generate_parameter_combinations(parameter_ranges: Dict[str, List[float]]) -> List[Dict[str, float]]:
    """ç”Ÿæˆå‚æ•°ç»„åˆ"""
    import itertools

    # è·å–å‚æ•°åç§°å’Œå€¼
    param_names = list(parameter_ranges.keys())
    param_values = list(parameter_ranges.values())

    # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
    combinations = []
    for combination in itertools.product(*param_values):
        param_dict = {}
        for name, value in zip(param_names, combination):
            # å¯¹äºçª—å£å‚æ•°ï¼Œè½¬æ¢ä¸ºæ•´æ•°
            if 'window' in name.lower():
                param_dict[name] = int(value)
            else:
                param_dict[name] = value
        combinations.append(param_dict)

    return combinations

def analyze_parameter_space(results: List[BacktestResult], metric: str = "sharpe_ratio") -> Dict:
    """åˆ†æå‚æ•°ç©ºé—´ï¼Œç”Ÿæˆå‚æ•°å¹³åŸæˆ–çƒ­åŠ›å›¾æ•°æ®"""
    if not results:
        return {"parameter_count": 0, "visualization_type": "none", "data": []}

    # æå–å‚æ•°åç§°
    first_result = results[0]
    if not first_result.parameters:
        return {"parameter_count": 0, "visualization_type": "none", "data": []}

    param_names = list(first_result.parameters.keys())
    param_count = len(param_names)

    # è·å–æŒ‡æ ‡å€¼çš„å‡½æ•°
    def get_metric_value(result: BacktestResult) -> float:
        if metric == "sharpe_ratio":
            return result.sharpe_ratio or 0
        elif metric == "final_return":
            return result.final_return or 0
        elif metric == "max_drawdown":
            return -(result.max_drawdown or 0)  # è´Ÿå€¼ï¼Œå› ä¸ºå›æ’¤è¶Šå°è¶Šå¥½
        elif metric == "win_rate":
            return result.win_rate or 0
        elif metric == "total_trades":
            return result.total_trades or 0
        else:
            return result.sharpe_ratio or 0

    if param_count == 1:
        # å•å‚æ•°ï¼šç”Ÿæˆå‚æ•°å¹³åŸæ•°æ®
        param_name = param_names[0]
        data_points = []

        for result in results:
            param_value = result.parameters[param_name]
            metric_value = get_metric_value(result)
            data_points.append({
                "parameter": param_value,
                "value": metric_value,
                "result": result
            })

        # æŒ‰å‚æ•°å€¼æ’åº
        data_points.sort(key=lambda x: x["parameter"])

        return {
            "parameter_count": 1,
            "visualization_type": "line_chart",
            "parameter_name": param_name,
            "data": data_points,
            "best_parameter": max(data_points, key=lambda x: x["value"])["parameter"],
            "best_value": max(data_points, key=lambda x: x["value"])["value"]
        }

    elif param_count == 2:
        # åŒå‚æ•°ï¼šç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
        param1_name, param2_name = param_names[0], param_names[1]

        # æ”¶é›†æ‰€æœ‰å‚æ•°å€¼
        param1_values = sorted(set(result.parameters[param1_name] for result in results))
        param2_values = sorted(set(result.parameters[param2_name] for result in results))

        # åˆ›å»ºçƒ­åŠ›å›¾çŸ©é˜µ
        heatmap_data = []
        best_value = float('-inf')
        best_params = None

        for p1_val in param1_values:
            row = []
            for p2_val in param2_values:
                # æŸ¥æ‰¾å¯¹åº”çš„ç»“æœ
                matching_result = None
                for result in results:
                    if (result.parameters[param1_name] == p1_val and
                        result.parameters[param2_name] == p2_val):
                        matching_result = result
                        break

                if matching_result:
                    metric_value = get_metric_value(matching_result)
                    row.append({
                        "value": metric_value,
                        "parameters": {param1_name: p1_val, param2_name: p2_val},
                        "result": matching_result
                    })

                    if metric_value > best_value:
                        best_value = metric_value
                        best_params = {param1_name: p1_val, param2_name: p2_val}
                else:
                    row.append({"value": None, "parameters": {param1_name: p1_val, param2_name: p2_val}})

            heatmap_data.append(row)

        return {
            "parameter_count": 2,
            "visualization_type": "heatmap",
            "parameter_names": [param1_name, param2_name],
            "parameter1_values": param1_values,
            "parameter2_values": param2_values,
            "heatmap_data": heatmap_data,
            "best_parameters": best_params,
            "best_value": best_value
        }

    else:
        # å¤šå‚æ•°ï¼šè¿”å›è¡¨æ ¼æ•°æ®
        data_points = []
        for result in results:
            metric_value = get_metric_value(result)
            data_points.append({
                "parameters": result.parameters,
                "value": metric_value,
                "result": result
            })

        # æŒ‰æŒ‡æ ‡å€¼æ’åº
        data_points.sort(key=lambda x: x["value"], reverse=True)

        return {
            "parameter_count": param_count,
            "visualization_type": "table",
            "parameter_names": param_names,
            "data": data_points[:20],  # åªè¿”å›å‰20ä¸ªç»“æœ
            "best_parameters": data_points[0]["parameters"] if data_points else None,
            "best_value": data_points[0]["value"] if data_points else None
        }

@router.get("/tasks/{task_id}/optimization-results")
async def get_optimization_results(
    task_id: str,
    sort_by: str = "sharpe_ratio",
    order: str = "desc",
    limit: int = 10
):
    """è·å–å‚æ•°ä¼˜åŒ–ç»“æœï¼Œæ”¯æŒæ’åºå’Œé™åˆ¶æ•°é‡"""
    if task_id not in backtest_tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    task = backtest_tasks[task_id]
    if not task.results:
        return {"message": "No optimization results available", "results": []}

    # å¤åˆ¶ç»“æœåˆ—è¡¨ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    results = task.results.copy()

    # æ’åº
    reverse = (order.lower() == "desc")
    if sort_by == "sharpe_ratio":
        results.sort(key=lambda x: x.sharpe_ratio or 0, reverse=reverse)
    elif sort_by == "final_return":
        results.sort(key=lambda x: x.final_return or 0, reverse=reverse)
    elif sort_by == "max_drawdown":
        results.sort(key=lambda x: x.max_drawdown or 0, reverse=not reverse)  # å›æ’¤è¶Šå°è¶Šå¥½
    elif sort_by == "win_rate":
        results.sort(key=lambda x: x.win_rate or 0, reverse=reverse)
    elif sort_by == "total_trades":
        results.sort(key=lambda x: x.total_trades or 0, reverse=reverse)

    # é™åˆ¶æ•°é‡
    limited_results = results[:limit]

    return {
        "task_id": task_id,
        "total_combinations": len(task.results),
        "showing": len(limited_results),
        "sort_by": sort_by,
        "order": order,
        "results": limited_results
    }

@router.get("/tasks/{task_id}/parameter-analysis")
async def get_parameter_analysis(
    task_id: str,
    metric: str = "sharpe_ratio"
):
    """è·å–å‚æ•°åˆ†ææ•°æ®ï¼Œæ”¯æŒå‚æ•°å¹³åŸå’Œçƒ­åŠ›å›¾"""
    if task_id not in backtest_tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    task = backtest_tasks[task_id]
    if not task.results:
        return {"message": "No optimization results available", "analysis": None}

    # åˆ†æå‚æ•°ç»´åº¦
    analysis = analyze_parameter_space(task.results, metric)

    return {
        "task_id": task_id,
        "metric": metric,
        "parameter_count": analysis["parameter_count"],
        "visualization_type": analysis["visualization_type"],
        "analysis": analysis
    }

@router.delete("/tasks/{task_id}")
async def delete_backtest_task(task_id: str):
    """åˆ é™¤å›æµ‹ä»»åŠ¡"""
    if task_id not in backtest_tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    del backtest_tasks[task_id]
    return {"message": "Task deleted successfully"}


@router.get("/data-range/{symbol}")
async def get_data_time_range(symbol: str, interval: str = "1H"):
    """è·å–æŒ‡å®šäº¤æ˜“å¯¹çš„å¯ç”¨æ•°æ®æ—¶é—´èŒƒå›´ - ä½¿ç”¨æœ¬åœ°æ•°æ®ç®¡ç†å™¨"""
    import logging
    logger = logging.getLogger(__name__)

    try:
        from ..services.data_adapter import data_adapter

        logger.info(f"ğŸ” æ£€æŸ¥ {symbol} æ•°æ®æ—¶é—´èŒƒå›´")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨è·å–æ•°æ®ä¿¡æ¯
        # ä¼˜å…ˆæ£€æŸ¥spotæ•°æ®
        spot_info = data_adapter.data_manager.get_data_info(symbol, "spot")
        swap_info = data_adapter.data_manager.get_data_info(symbol, "swap")

        # é€‰æ‹©æœ€ä½³æ•°æ®æº
        best_info = None
        data_source = None

        if spot_info["available"] and swap_info["available"]:
            # ä¸¤ç§æ•°æ®éƒ½å¯ç”¨ï¼Œé€‰æ‹©è®°å½•æ•°æ›´å¤šçš„
            if spot_info["records_count"] >= swap_info["records_count"]:
                best_info = spot_info
                data_source = "spot"
            else:
                best_info = swap_info
                data_source = "swap"
        elif spot_info["available"]:
            best_info = spot_info
            data_source = "spot"
        elif swap_info["available"]:
            best_info = swap_info
            data_source = "swap"

        if best_info and best_info["available"]:
            # è®¡ç®—è´¨é‡è¯„åˆ†
            quality_score = "high" if best_info["records_count"] >= 5000 else \
                           "medium" if best_info["records_count"] >= 1000 else "low"

            result = {
                "symbol": symbol,
                "available": True,
                "message": f"ä½¿ç”¨æœ¬åœ°é¢„å¤„ç†æ•°æ® ({data_source})",
                "start_date": best_info["time_range"]["start"],
                "end_date": best_info["time_range"]["end"],
                "total_records": best_info["records_count"],
                "records_count": best_info["records_count"],
                "time_range": best_info["time_range"],
                "quality_score": quality_score,
                "recommended": True,
                "quality_message": f"æœ¬åœ°æ•°æ®å¯ç”¨ï¼Œ{best_info['records_count']:,} æ¡è®°å½•",
                "data_source": f"local_preprocess_data_{data_source}",
                "price_range": best_info.get("price_range")
            }

            logger.info(f"âœ… {symbol}: æœ¬åœ°æ•°æ®å¯ç”¨ - {best_info['records_count']:,} æ¡è®°å½•")
            return result
        else:
            # æ•°æ®ä¸å¯ç”¨
            logger.warning(f"âš ï¸ {symbol}: æœ¬åœ°æ•°æ®ä¸å¯ç”¨")
            return {
                "symbol": symbol,
                "available": False,
                "message": f"äº¤æ˜“å¯¹ {symbol} åœ¨æœ¬åœ°æ•°æ®ä¸­ä¸å¯ç”¨",
                "start_date": None,
                "end_date": None,
                "total_records": 0,
                "records_count": 0,
                "time_range": None,
                "quality_score": "unavailable",
                "recommended": False,
                "quality_message": "æœ¬åœ°æ•°æ®ä¸­æœªæ‰¾åˆ°è¯¥äº¤æ˜“å¯¹",
                "data_source": "local_preprocess_data"
            }

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®èŒƒå›´å¤±è´¥ {symbol}: {e}")
        return {
            "symbol": symbol,
            "available": False,
            "message": f"è·å–æ•°æ®èŒƒå›´å¤±è´¥: {str(e)}",
            "start_date": None,
            "end_date": None,
            "total_records": 0,
            "records_count": 0,
            "time_range": None,
            "quality_score": "error",
            "recommended": False,
            "quality_message": f"æ•°æ®è·å–å¤±è´¥: {str(e)}",
            "data_source": "local_preprocess_data"
        }


def assess_data_file_quality(df, symbol: str, interval: str) -> dict:
    """è¯„ä¼°æ•°æ®æ–‡ä»¶è´¨é‡"""
    import pandas as pd
    from datetime import datetime, timedelta

    try:
        if df is None or df.empty:
            return {
                'quality_score': 0,
                'quality_message': 'æ•°æ®æ–‡ä»¶ä¸ºç©º',
                'start_date': None,
                'end_date': None,
                'total_records': 0
            }

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_columns = ['candle_begin_time', 'open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            return {
                'quality_score': 1,
                'quality_message': f'ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}',
                'start_date': None,
                'end_date': None,
                'total_records': len(df)
            }

        # è·å–æ—¶é—´èŒƒå›´
        start_date = df['candle_begin_time'].min()
        end_date = df['candle_begin_time'].max()
        total_records = len(df)

        # è®¡ç®—è´¨é‡è¯„åˆ† (1-5åˆ†)
        quality_score = 1
        quality_messages = []

        # æ•°æ®é‡è¯„åˆ†
        if total_records >= 5000:
            quality_score = 5
            quality_messages.append('æ•°æ®å……è¶³')
        elif total_records >= 2000:
            quality_score = 4
            quality_messages.append('æ•°æ®é‡è‰¯å¥½')
        elif total_records >= 1000:
            quality_score = 3
            quality_messages.append('æ•°æ®é‡é€‚ä¸­')
        elif total_records >= 500:
            quality_score = 2
            quality_messages.append('æ•°æ®é‡è¾ƒå°‘')
        else:
            quality_score = 1
            quality_messages.append('æ•°æ®é‡ä¸è¶³')

        # æ—¶é—´è·¨åº¦æ£€æŸ¥
        time_span = (end_date - start_date).days
        if time_span < 30:
            quality_score = max(1, quality_score - 1)
            quality_messages.append('æ—¶é—´è·¨åº¦è¾ƒçŸ­')
        elif time_span >= 365:
            quality_messages.append('æ—¶é—´è·¨åº¦å……è¶³')

        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        null_count = df[required_columns].isnull().sum().sum()
        if null_count > 0:
            quality_score = max(1, quality_score - 1)
            quality_messages.append(f'å­˜åœ¨{null_count}ä¸ªç©ºå€¼')

        # ä»·æ ¼èŒƒå›´
        price_range = None
        if 'close' in df.columns:
            price_range = {
                'min': float(df['close'].min()),
                'max': float(df['close'].max())
            }

        return {
            'quality_score': quality_score,
            'quality_message': ', '.join(quality_messages),
            'start_date': start_date.strftime('%Y-%m-%d'),
            'end_date': end_date.strftime('%Y-%m-%d'),
            'total_records': total_records,
            'price_range': price_range
        }

    except Exception as e:
        return {
            'quality_score': 0,
            'quality_message': f'æ•°æ®è¯„ä¼°å¤±è´¥: {str(e)}',
            'start_date': None,
            'end_date': None,
            'total_records': 0
        }


def map_quality_score(numeric_score: int) -> str:
    """å°†æ•°å­—è´¨é‡è¯„åˆ†æ˜ å°„ä¸ºå­—ç¬¦ä¸²"""
    if numeric_score >= 5:
        return 'high'
    elif numeric_score >= 3:
        return 'medium'
    elif numeric_score >= 1:
        return 'low'
    else:
        return 'unavailable'


async def get_symbol_info_from_api(symbol: str) -> dict:
    """ä»APIè·å–äº¤æ˜“å¯¹ä¿¡æ¯"""
    try:
        import aiohttp

        # å°è¯•ä»Binance APIè·å–äº¤æ˜“å¯¹ä¿¡æ¯
        url = f"https://fapi.binance.com/fapi/v1/exchangeInfo"

        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    symbols = data.get('symbols', [])

                    for s in symbols:
                        if s.get('symbol') == symbol:
                            return {
                                'symbol': symbol,
                                'status': s.get('status'),
                                'available_from_api': True
                            }

                    return {
                        'symbol': symbol,
                        'status': 'NOT_FOUND',
                        'available_from_api': False
                    }
                else:
                    return {
                        'symbol': symbol,
                        'status': 'API_ERROR',
                        'available_from_api': False
                    }

    except Exception as e:
        return {
            'symbol': symbol,
            'status': 'CONNECTION_ERROR',
            'available_from_api': False,
            'error': str(e)
        }


@router.post("/prepare-data-on-demand")
async def prepare_data_on_demand(request: BacktestRequest):
    """æŒ‰éœ€æ•°æ®å‡†å¤‡API - æ ¹æ®å›æµ‹é…ç½®è‡ªåŠ¨è·å–å’Œå‡†å¤‡æ•°æ®"""
    import logging
    import asyncio
    from datetime import datetime, timedelta

    logger = logging.getLogger(__name__)

    try:
        logger.info(f"ğŸš€ å¼€å§‹æŒ‰éœ€æ•°æ®å‡†å¤‡")
        logger.info(f"   äº¤æ˜“å¯¹: {request.symbols}")
        logger.info(f"   æ—¶é—´èŒƒå›´: {request.date_start} åˆ° {request.date_end}")
        logger.info(f"   æ—¶é—´å‘¨æœŸ: {request.rule_type}")

        # åˆå§‹åŒ–ç»“æœ
        preparation_result = {
            "status": "processing",
            "task_id": f"data_prep_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "symbols_total": len(request.symbols),
            "symbols_processed": 0,
            "symbols_ready": 0,
            "symbols_updated": 0,
            "data_sources": {},
            "time_range_info": {
                "requested_start": request.date_start,
                "requested_end": request.date_end,
                "actual_start": request.date_start,
                "actual_end": request.date_end
            },
            "progress": 0,
            "message": "å¼€å§‹æ•°æ®å‡†å¤‡...",
            "warnings": [],
            "errors": []
        }

        # éªŒè¯æ—¶é—´èŒƒå›´
        try:
            start_date = datetime.strptime(request.date_start, '%Y-%m-%d')
            end_date = datetime.strptime(request.date_end, '%Y-%m-%d')

            if start_date >= end_date:
                raise ValueError("å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºç»“æŸæ—¥æœŸ")

            if (end_date - start_date).days < 7:
                preparation_result["warnings"].append("æ—¶é—´èŒƒå›´è¾ƒçŸ­ï¼Œå»ºè®®è‡³å°‘é€‰æ‹©7å¤©ä»¥ä¸Šçš„æ•°æ®")

        except ValueError as e:
            preparation_result["status"] = "error"
            preparation_result["errors"].append(f"æ—¶é—´èŒƒå›´æ ¼å¼é”™è¯¯: {str(e)}")
            return preparation_result

        # å¤„ç†æ¯ä¸ªäº¤æ˜“å¯¹
        for i, symbol in enumerate(request.symbols):
            logger.info(f"ğŸ“Š å¤„ç†äº¤æ˜“å¯¹ {symbol} ({i+1}/{len(request.symbols)})")

            try:
                # æ£€æŸ¥å¹¶å‡†å¤‡æ•°æ®
                data_info = await prepare_symbol_data_on_demand(symbol, request)
                preparation_result["data_sources"][symbol] = data_info

                if data_info["status"] == "ready":
                    preparation_result["symbols_ready"] += 1
                elif data_info["status"] == "updated":
                    preparation_result["symbols_updated"] += 1
                    preparation_result["symbols_ready"] += 1

                preparation_result["symbols_processed"] += 1
                preparation_result["progress"] = int((i + 1) / len(request.symbols) * 100)
                preparation_result["message"] = f"å·²å¤„ç† {i+1}/{len(request.symbols)} ä¸ªäº¤æ˜“å¯¹"

            except Exception as e:
                logger.error(f"âŒ å¤„ç† {symbol} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                preparation_result["errors"].append(f"{symbol}: {str(e)}")
                preparation_result["data_sources"][symbol] = {
                    "status": "error",
                    "error": str(e)
                }

        # å®ŒæˆçŠ¶æ€æ›´æ–°
        if preparation_result["symbols_ready"] == len(request.symbols):
            preparation_result["status"] = "completed"
            preparation_result["message"] = f"æ•°æ®å‡†å¤‡å®Œæˆï¼Œ{preparation_result['symbols_ready']} ä¸ªäº¤æ˜“å¯¹å°±ç»ª"
        elif preparation_result["symbols_ready"] > 0:
            preparation_result["status"] = "partial_success"
            preparation_result["message"] = f"éƒ¨åˆ†æ•°æ®å‡†å¤‡å®Œæˆï¼Œ{preparation_result['symbols_ready']}/{len(request.symbols)} ä¸ªäº¤æ˜“å¯¹å°±ç»ª"
        else:
            preparation_result["status"] = "failed"
            preparation_result["message"] = "æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œæ²¡æœ‰å¯ç”¨çš„äº¤æ˜“å¯¹æ•°æ®"

        logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {preparation_result['status']}")
        return preparation_result

    except Exception as e:
        logger.error(f"âŒ æŒ‰éœ€æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
        return {
            "status": "error",
            "message": f"æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
            "errors": [str(e)]
        }

async def prepare_symbol_data_on_demand(symbol: str, request: BacktestRequest) -> dict:
    """æ£€æŸ¥å•ä¸ªäº¤æ˜“å¯¹çš„æœ¬åœ°æ•°æ®å¯ç”¨æ€§"""
    import logging
    from datetime import datetime

    logger = logging.getLogger(__name__)

    try:
        # ä½¿ç”¨æœ¬åœ°æ•°æ®ç®¡ç†å™¨æ£€æŸ¥æ•°æ®
        from ..services.data_adapter import data_adapter

        logger.info(f"ğŸ” {symbol}: æ£€æŸ¥æœ¬åœ°æ•°æ®å¯ç”¨æ€§")

        data_info = {
            "symbol": symbol,
            "status": "checking",
            "data_source": "local_preprocess_data",
            "records_count": 0,
            "time_range": {},
            "quality_score": 0,
            "needs_update": False,
            "update_reason": ""
        }

        # æ£€æŸ¥æœ¬åœ°æ•°æ®å¯ç”¨æ€§
        df = data_adapter.get_symbol_data_for_backtest(
            symbol, request.date_start, request.date_end, request.rule_type
        )

        if df is not None and not df.empty:
            # æ•°æ®å¯ç”¨
            data_info.update({
                "status": "ready",
                "records_count": len(df),
                "data_source": "local_preprocess_data",
                "quality_score": calculate_data_quality_score(df)
            })

            # è·å–æ—¶é—´èŒƒå›´
            if 'candle_begin_time' in df.columns:
                data_info["time_range"] = {
                    "start": df['candle_begin_time'].min().strftime('%Y-%m-%d'),
                    "end": df['candle_begin_time'].max().strftime('%Y-%m-%d')
                }

            logger.info(f"âœ… {symbol}: æœ¬åœ°æ•°æ®å¯ç”¨ï¼Œ{len(df)} æ¡è®°å½•")
            return data_info
        else:
            # æ•°æ®ä¸å¯ç”¨
            data_info.update({
                "status": "unavailable",
                "update_reason": "æœ¬åœ°é¢„å¤„ç†æ•°æ®ä¸­æœªæ‰¾åˆ°è¯¥äº¤æ˜“å¯¹",
                "message": f"äº¤æ˜“å¯¹ {symbol} åœ¨æœ¬åœ°æ•°æ®ä¸­ä¸å¯ç”¨"
            })
            logger.warning(f"âš ï¸ {symbol}: æœ¬åœ°æ•°æ®ä¸å¯ç”¨")
            return data_info



    except Exception as e:
        logger.error(f"âŒ {symbol}: æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return {
            "symbol": symbol,
            "status": "error",
            "error": str(e)
        }

def get_data_file_path(symbol: str, rule_type: str) -> str:
    """è·å–æ•°æ®æ–‡ä»¶è·¯å¾„"""
    import os

    # æ ¹æ®æ—¶é—´å‘¨æœŸæ˜ å°„åˆ°å¯¹åº”çš„ç›®å½•
    interval_map = {
        '1m': '1m',
        '5m': '5m',
        '15m': '15m',
        '1H': '1H',
        '4H': '4H',
        '1D': '1D'
    }

    interval = interval_map.get(rule_type, '1H')

    # æ„å»ºpklæ–‡ä»¶è·¯å¾„
    base_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'bn_data', 'output', 'pickle_data')
    file_path = os.path.join(base_path, interval, f"{symbol}.pkl")

    return os.path.abspath(file_path)

def calculate_data_quality_score(df) -> int:
    """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ† (0-100)"""
    try:
        if df is None or df.empty:
            return 0

        score = 0

        # æ•°æ®é‡è¯„åˆ† (40åˆ†)
        record_count = len(df)
        if record_count >= 5000:
            score += 40
        elif record_count >= 2000:
            score += 30
        elif record_count >= 1000:
            score += 20
        elif record_count >= 500:
            score += 10

        # æ•°æ®å®Œæ•´æ€§è¯„åˆ† (30åˆ†)
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        available_columns = [col for col in required_columns if col in df.columns]
        if len(available_columns) == len(required_columns):
            score += 30
        else:
            score += int(30 * len(available_columns) / len(required_columns))

        # æ•°æ®è´¨é‡è¯„åˆ† (30åˆ†)
        if len(available_columns) > 0:
            # æ£€æŸ¥ç©ºå€¼æ¯”ä¾‹
            null_ratio = df[available_columns].isnull().sum().sum() / (len(df) * len(available_columns))
            if null_ratio < 0.01:  # ç©ºå€¼å°‘äº1%
                score += 30
            elif null_ratio < 0.05:  # ç©ºå€¼å°‘äº5%
                score += 20
            elif null_ratio < 0.1:  # ç©ºå€¼å°‘äº10%
                score += 10

        return min(score, 100)

    except Exception:
        return 0

async def update_symbol_data_via_bn_data(symbol: str, request: BacktestRequest) -> bool:
    """é€šè¿‡bn_dataæ¨¡å—æ›´æ–°å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®"""
    import subprocess
    import os
    import logging
    from datetime import datetime, timedelta

    logger = logging.getLogger(__name__)

    try:
        logger.info(f"ğŸ”„ å¼€å§‹é€šè¿‡bn_dataæ›´æ–° {symbol} æ•°æ®")

        # è·å–bn_dataè·¯å¾„
        bn_data_path = os.path.join(os.path.dirname(__file__), '..', '..', '..', 'bn_data')
        if not os.path.exists(bn_data_path):
            logger.error(f"âŒ bn_dataç›®å½•ä¸å­˜åœ¨: {bn_data_path}")
            return False

        # æ£€æŸ¥bn_dataé…ç½®
        config_path = os.path.join(bn_data_path, 'config.py')
        if not os.path.exists(config_path):
            logger.error(f"âŒ bn_dataé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False

        # ç¡®ä¿é…ç½®æ­£ç¡®
        await ensure_bn_data_config(config_path)

        # åˆ‡æ¢åˆ°bn_dataç›®å½•
        original_cwd = os.getcwd()
        os.chdir(bn_data_path)

        try:
            logger.info(f"   æ‰§è¡Œç›®å½•: {bn_data_path}")
            logger.info(f"   ç›®æ ‡äº¤æ˜“å¯¹: {symbol}")
            logger.info(f"   æ—¶é—´èŒƒå›´: {request.date_start} åˆ° {request.date_end}")

            # è¿è¡Œbn_dataä¸»ç¨‹åºè¿›è¡Œæ•°æ®æ›´æ–°
            # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
            result = subprocess.run(
                ['python', 'main.py'],
                capture_output=True,
                text=True,
                timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶
                cwd=bn_data_path
            )

            if result.returncode == 0:
                logger.info(f"âœ… {symbol}: bn_dataæ‰§è¡ŒæˆåŠŸ")
                logger.info(f"   è¾“å‡ºæ‘˜è¦: {result.stdout[-500:] if result.stdout else 'æ— è¾“å‡º'}")

                # éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
                data_file_path = get_data_file_path(symbol, request.rule_type)
                if os.path.exists(data_file_path):
                    logger.info(f"âœ… {symbol}: æ•°æ®æ–‡ä»¶å·²ç”Ÿæˆ {data_file_path}")
                    return True
                else:
                    logger.warning(f"âš ï¸ {symbol}: bn_dataæ‰§è¡ŒæˆåŠŸä½†æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°")
                    return False
            else:
                logger.error(f"âŒ {symbol}: bn_dataæ‰§è¡Œå¤±è´¥")
                logger.error(f"   é”™è¯¯è¾“å‡º: {result.stderr}")
                return False

        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {symbol}: bn_dataæ‰§è¡Œè¶…æ—¶")
            return False
        except Exception as e:
            logger.error(f"âŒ {symbol}: bn_dataæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return False
        finally:
            os.chdir(original_cwd)

    except Exception as e:
        logger.error(f"âŒ {symbol}: æ•°æ®æ›´æ–°è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        return False

async def ensure_bn_data_config(config_path: str):
    """ç¡®ä¿bn_dataé…ç½®æ­£ç¡®"""
    import logging

    logger = logging.getLogger(__name__)

    try:
        # è¯»å–å½“å‰é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config_content = f.read()

        # æ£€æŸ¥å…³é”®é…ç½®
        if 'update_to_now = True' not in config_content:
            logger.info("ğŸ”§ æ›´æ–°bn_dataé…ç½®: å¯ç”¨update_to_now")
            # æ›¿æ¢é…ç½®
            config_content = config_content.replace(
                'update_to_now = False',
                'update_to_now = True'
            )

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®è¡Œï¼Œæ·»åŠ é…ç½®
            if 'update_to_now' not in config_content:
                config_content += '\n# è‡ªåŠ¨æ·»åŠ çš„é…ç½®\nupdate_to_now = True\n'

            # å†™å›é…ç½®æ–‡ä»¶
            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(config_content)

            logger.info("âœ… bn_dataé…ç½®å·²æ›´æ–°")
        else:
            logger.info("âœ… bn_dataé…ç½®å·²æ­£ç¡®")

    except Exception as e:
        logger.warning(f"âš ï¸ æ£€æŸ¥bn_dataé…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

# ä¿®æ”¹ä¸»å›æµ‹æ¥å£ï¼Œé›†æˆæŒ‰éœ€æ•°æ®è·å–
@router.post("/run-with-auto-data")
async def run_backtest_with_auto_data(request: BacktestRequest, background_tasks: BackgroundTasks):
    """è¿è¡Œå›æµ‹ - ä½¿ç”¨æœ¬åœ°æ•°æ®æºï¼Œåˆ›å»ºåå°ä»»åŠ¡"""
    import logging
    import uuid
    from datetime import datetime

    logger = logging.getLogger(__name__)

    try:
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        logger.info(f"ğŸš€ å¼€å§‹å›æµ‹ä»»åŠ¡: {task_id}")

        # åˆ›å»ºä»»åŠ¡çŠ¶æ€
        task_status = BacktestStatus(
            task_id=task_id,
            status="pending",
            message="å›æµ‹ä»»åŠ¡å·²åˆ›å»º - ä½¿ç”¨æœ¬åœ°æ•°æ®",
            symbols_total=len(request.symbols)
        )
        backtest_tasks[task_id] = task_status

        # åœ¨åå°è¿è¡Œå›æµ‹
        background_tasks.add_task(run_backtest_with_auto_data_task, task_id, request)

        return {
            "task_id": task_id,
            "message": "å›æµ‹å·²å¯åŠ¨ - ä½¿ç”¨æœ¬åœ°æ•°æ®",
            "status": "pending"
        }

    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å›æµ‹ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å›æµ‹å¤±è´¥: {str(e)}")


async def run_backtest_with_auto_data_task(task_id: str, request: BacktestRequest):
    """åå°è¿è¡Œå›æµ‹ä»»åŠ¡ - ä½¿ç”¨æœ¬åœ°æ•°æ®æº"""
    import logging
    logger = logging.getLogger(__name__)

    task_status = backtest_tasks[task_id]

    try:
        task_status.status = "running"
        task_status.message = "æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ•°æ®è¿›è¡Œå›æµ‹..."

        logger.info("ğŸ“Š ä½¿ç”¨æœ¬åœ°é¢„å¤„ç†æ•°æ®è¿›è¡Œå›æµ‹")

        # éªŒè¯æœ¬åœ°æ•°æ®å¯ç”¨æ€§
        from ..services.data_adapter import data_adapter
        data_availability_check = {}

        for symbol in request.symbols:
            df = data_adapter.get_symbol_data_for_backtest(
                symbol, request.date_start, request.date_end, request.rule_type
            )
            data_availability_check[symbol] = {
                "available": df is not None and not df.empty,
                "records": len(df) if df is not None else 0
            }

        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ•°æ®
        available_symbols = [s for s, info in data_availability_check.items() if info["available"]]

        if not available_symbols:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æœ¬åœ°æ•°æ®")
            task_status.status = "failed"
            task_status.message = "æœªæ‰¾åˆ°å¯ç”¨çš„æœ¬åœ°æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æºé…ç½®"
            return

        logger.info(f"âœ… æ•°æ®å¯ç”¨æ€§æ£€æŸ¥å®Œæˆ: {len(available_symbols)}/{len(request.symbols)} ä¸ªäº¤æ˜“å¯¹å¯ç”¨")

        # 2. ä½¿ç”¨å¯ç”¨çš„æœ¬åœ°æ•°æ®è¿›è¡Œå›æµ‹
        logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œå›æµ‹")

        # ä½¿ç”¨æœ‰æ•°æ®çš„äº¤æ˜“å¯¹è¿›è¡Œå›æµ‹
        ready_symbols = available_symbols

        if not ready_symbols:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„äº¤æ˜“å¯¹æ•°æ®")
            task_status.status = "failed"
            task_status.message = "æ²¡æœ‰å¯ç”¨çš„äº¤æ˜“å¯¹æ•°æ®è¿›è¡Œå›æµ‹"
            return

        # åˆ›å»ºæ–°çš„è¯·æ±‚å¯¹è±¡ï¼ŒåªåŒ…å«å°±ç»ªçš„äº¤æ˜“å¯¹
        filtered_request = BacktestRequest(
            symbols=ready_symbols,
            strategy=request.strategy,
            parameters=request.parameters,
            date_start=request.date_start,
            date_end=request.date_end,
            rule_type=request.rule_type,
            leverage_rate=request.leverage_rate,
            c_rate=request.c_rate,
            slippage=request.slippage
        )

        # æ‰§è¡Œå›æµ‹
        backtest_results = []
        for i, symbol in enumerate(ready_symbols):
            task_status.message = f"æ­£åœ¨å›æµ‹ {symbol}..."
            task_status.progress = (i / len(ready_symbols)) * 100
            task_status.symbols_completed = i

            logger.info(f"ğŸ”„ å›æµ‹ {symbol}")
            try:
                result = await run_real_backtest(symbol, filtered_request)
                if result:
                    result.task_id = task_id
                    backtest_results.append(result)
                    logger.info(f"âœ… {symbol}: å›æµ‹å®Œæˆ")
                else:
                    logger.warning(f"âš ï¸ {symbol}: å›æµ‹å¤±è´¥")
            except Exception as e:
                logger.error(f"âŒ {symbol}: å›æµ‹å¼‚å¸¸: {str(e)}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task_status.symbols_completed = len(ready_symbols)
        task_status.progress = 100
        task_status.results = backtest_results

        if backtest_results:
            task_status.status = "completed"
            task_status.message = f"å›æµ‹å®Œæˆï¼Œè·å¾— {len(backtest_results)} ä¸ªç»“æœ"
            logger.info(f"âœ… å›æµ‹å®Œæˆ: {len(backtest_results)} ä¸ªç»“æœ")
        else:
            task_status.status = "failed"
            task_status.message = "æ‰€æœ‰å›æµ‹éƒ½å¤±è´¥äº†"
            logger.error("âŒ æ‰€æœ‰å›æµ‹éƒ½å¤±è´¥äº†")

    except Exception as e:
        logger.error(f"âŒ è‡ªåŠ¨æ•°æ®å›æµ‹è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        task_status.status = "failed"
        task_status.message = f"å›æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@router.post("/prepare-data")
async def prepare_backtest_data(
    symbols: List[str],
    date_start: str,
    date_end: str,
    rule_type: str = "1H"
):
    """æ•°æ®é¢„å¤„ç†API - æ£€æŸ¥å’Œå‡†å¤‡å›æµ‹æ‰€éœ€çš„æ•°æ®ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    import logging
    logger = logging.getLogger(__name__)

    try:
        logger.info(f"ğŸ” å¼€å§‹æ•°æ®é¢„å¤„ç†: {symbols}, {date_start} åˆ° {date_end}, å‘¨æœŸ: {rule_type}")

        preparation_result = {
            "status": "success",
            "symbols_checked": len(symbols),
            "symbols_ready": 0,
            "symbols_missing": 0,
            "data_quality": {},
            "time_range_adjusted": False,
            "adjusted_start": date_start,
            "adjusted_end": date_end,
            "recommendations": [],
            "warnings": []
        }

        symbols_ready = []
        symbols_missing = []

        for symbol in symbols:
            # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§å’Œè´¨é‡
            data_info = await get_data_time_range(symbol, rule_type)

            if data_info["available"]:
                symbols_ready.append(symbol)
                preparation_result["data_quality"][symbol] = {
                    "status": "ready",
                    "quality_score": data_info.get("quality_score", "medium"),
                    "total_records": data_info.get("total_records", 0),
                    "time_range": f"{data_info.get('start_date')} åˆ° {data_info.get('end_date')}"
                }

                # æ£€æŸ¥æ˜¯å¦æ¨èä½¿ç”¨
                if data_info.get("recommended", False):
                    preparation_result["recommendations"].append(f"âœ… {symbol}: æ•°æ®è´¨é‡ä¼˜è‰¯ï¼Œæ¨èä½¿ç”¨")
                else:
                    preparation_result["warnings"].append(f"âš ï¸ {symbol}: {data_info.get('quality_message', 'æ•°æ®è´¨é‡ä¸€èˆ¬')}")
            else:
                symbols_missing.append(symbol)
                preparation_result["data_quality"][symbol] = {
                    "status": "missing",
                    "message": data_info.get("message", "æ•°æ®ä¸å¯ç”¨")
                }
                preparation_result["warnings"].append(f"âŒ {symbol}: æ•°æ®ä¸å¯ç”¨ - {data_info.get('message', '')}")

        preparation_result["symbols_ready"] = len(symbols_ready)
        preparation_result["symbols_missing"] = len(symbols_missing)

        # æ™ºèƒ½æ—¶é—´èŒƒå›´è°ƒæ•´å»ºè®®
        if symbols_ready:
            # è®¡ç®—æ‰€æœ‰å¯ç”¨æ•°æ®çš„é‡å æ—¶é—´èŒƒå›´
            overlapping_range = calculate_overlapping_time_range(symbols_ready, rule_type)

            if overlapping_range:
                user_start = datetime.strptime(date_start, '%Y-%m-%d')
                user_end = datetime.strptime(date_end, '%Y-%m-%d')
                data_start = datetime.strptime(overlapping_range['start'], '%Y-%m-%d')
                data_end = datetime.strptime(overlapping_range['end'], '%Y-%m-%d')

                adjusted_start = max(user_start, data_start)
                adjusted_end = min(user_end, data_end)

                if adjusted_start != user_start or adjusted_end != user_end:
                    preparation_result["time_range_adjusted"] = True
                    preparation_result["adjusted_start"] = adjusted_start.strftime('%Y-%m-%d')
                    preparation_result["adjusted_end"] = adjusted_end.strftime('%Y-%m-%d')
                    preparation_result["recommendations"].append(
                        f"ğŸ“… å»ºè®®è°ƒæ•´æ—¶é—´èŒƒå›´ä¸º: {preparation_result['adjusted_start']} åˆ° {preparation_result['adjusted_end']}"
                    )

        # ç”Ÿæˆæ€»ä½“å»ºè®®
        if preparation_result["symbols_ready"] == 0:
            preparation_result["status"] = "failed"
            preparation_result["recommendations"].append("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®ï¼Œè¯·æ£€æŸ¥äº¤æ˜“å¯¹é€‰æ‹©æˆ–è¿è¡Œæ•°æ®æ›´æ–°")
        elif preparation_result["symbols_missing"] > 0:
            preparation_result["status"] = "partial"
            preparation_result["recommendations"].append(f"âš ï¸ {preparation_result['symbols_missing']} ä¸ªäº¤æ˜“å¯¹æ•°æ®ç¼ºå¤±ï¼Œå»ºè®®è¿è¡Œæ•°æ®æ›´æ–°")
        else:
            preparation_result["recommendations"].append("âœ… æ‰€æœ‰æ•°æ®å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å›æµ‹")

        logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {preparation_result['symbols_ready']}/{preparation_result['symbols_checked']} ä¸ªäº¤æ˜“å¯¹å¯ç”¨")
        return preparation_result

    except Exception as e:
        logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}",
            "symbols_checked": len(symbols),
            "symbols_ready": 0,
            "symbols_missing": len(symbols)
        }


def calculate_overlapping_time_range(symbols: List[str], rule_type: str) -> dict:
    """è®¡ç®—å¤šä¸ªäº¤æ˜“å¯¹çš„æ•°æ®é‡å æ—¶é—´èŒƒå›´"""
    try:
        import pandas as pd
        import os
        from datetime import datetime

        start_dates = []
        end_dates = []

        for symbol in symbols:
            data_file = f"bn_data/output/pickle_data/{rule_type}/{symbol}.pkl"

            if os.path.exists(data_file):
                try:
                    df = pd.read_pickle(data_file)
                    if not df.empty and 'candle_begin_time' in df.columns:
                        start_dates.append(df['candle_begin_time'].min())
                        end_dates.append(df['candle_begin_time'].max())
                except Exception:
                    continue

        if start_dates and end_dates:
            overlapping_start = max(start_dates)
            overlapping_end = min(end_dates)

            if overlapping_start <= overlapping_end:
                return {
                    'start': overlapping_start.strftime('%Y-%m-%d'),
                    'end': overlapping_end.strftime('%Y-%m-%d'),
                    'days': (overlapping_end - overlapping_start).days
                }

        return None

    except Exception:
        return None


@router.get("/available-symbols")
async def get_available_symbols():
    """è·å–æ‰€æœ‰å¯ç”¨çš„äº¤æ˜“å¯¹åˆ—è¡¨"""
    try:
        from ..services.data_adapter import data_adapter

        logger = logging.getLogger(__name__)
        logger.info("ğŸ” è·å–å¯ç”¨äº¤æ˜“å¯¹åˆ—è¡¨...")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨è·å–äº¤æ˜“å¯¹
        symbols = await data_adapter.get_usdt_symbols_async()

        if not symbols:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„äº¤æ˜“å¯¹")
            return {
                "symbols": [],
                "message": "æœªæ‰¾åˆ°å¯ç”¨çš„äº¤æ˜“å¯¹ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•é…ç½®",
                "total_count": 0
            }

        symbols.sort()  # æŒ‰å­—æ¯é¡ºåºæ’åº

        logger.info(f"âœ… æ‰¾åˆ° {len(symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹")

        return {
            "symbols": symbols,
            "message": f"æ‰¾åˆ° {len(symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹",
            "total_count": len(symbols)
        }

    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"âŒ è·å–å¯ç”¨äº¤æ˜“å¯¹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¯ç”¨äº¤æ˜“å¯¹å¤±è´¥: {str(e)}")
