"""
æ•°æ®ç®¡ç†APIè·¯ç”±
ä½¿ç”¨æœ¬åœ°é¢„å¤„ç†æ•°æ®ï¼Œæ›¿ä»£bn_dataæ¨¡å—
"""
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict
import asyncio
import sys
import os
import logging
import pandas as pd
from datetime import datetime

# å¯¼å…¥æ–°çš„æ•°æ®æœåŠ¡
from ..services.data_adapter import data_adapter
from ..services.local_data_manager import local_data_manager

logger = logging.getLogger(__name__)

router = APIRouter()

# æ•°æ®æ¨¡å‹
class SymbolInfo(BaseModel):
    symbol: str
    status: str
    last_update: Optional[str] = None

class DataDownloadRequest(BaseModel):
    symbols: Optional[List[str]] = None
    trade_type: str = "swap"
    intervals: List[str] = ["1m", "5m"]

class DataStatus(BaseModel):
    status: str  # "idle", "downloading", "processing", "completed", "error"
    progress: float = 0.0
    message: str = ""
    symbols_total: int = 0
    symbols_completed: int = 0

# å…¨å±€çŠ¶æ€ç®¡ç†
data_status = DataStatus(status="idle", message="Ready to download data")

@router.get("/symbols", response_model=List[SymbolInfo])
async def get_symbols(trade_type: str = "spot"):
    """è·å–å¯ç”¨çš„äº¤æ˜“å¯¹åˆ—è¡¨"""
    try:
        logger.info(f"ğŸ“Š è·å–äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œå¸‚åœºç±»å‹: {trade_type}")

        # ä½¿ç”¨æ–°çš„æ•°æ®é€‚é…å™¨è·å–äº¤æ˜“å¯¹
        symbols = await data_adapter.get_usdt_symbols_async()

        # æ ¹æ®å¸‚åœºç±»å‹è¿‡æ»¤
        if trade_type == "spot":
            symbols = data_adapter.spot_symbols_filter(symbols)

        logger.info(f"âœ… è·å–åˆ° {len(symbols)} ä¸ª {trade_type} äº¤æ˜“å¯¹")

        symbol_list = []
        for symbol in symbols:
            symbol_list.append(SymbolInfo(
                symbol=symbol,
                status="available"
            ))

        return symbol_list
    except Exception as e:
        logger.error(f"âŒ è·å–äº¤æ˜“å¯¹åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get symbols: {str(e)}")

@router.get("/status", response_model=DataStatus)
async def get_data_status():
    """è·å–æ•°æ®ä¸‹è½½çŠ¶æ€"""
    return data_status

@router.post("/download")
async def start_data_download(
    request: DataDownloadRequest,
    background_tasks: BackgroundTasks
):
    """å¯åŠ¨æ•°æ®ä¸‹è½½ä»»åŠ¡"""
    global data_status

    if data_status.status == "downloading":
        raise HTTPException(status_code=400, detail="Data download already in progress")

    # æ›´æ–°çŠ¶æ€
    data_status.status = "downloading"
    data_status.progress = 0.0
    data_status.message = "Starting data download..."

    # åœ¨åå°è¿è¡Œæ•°æ®ä¸‹è½½
    background_tasks.add_task(run_data_download, request)

    return {"message": "Data download started", "status": data_status.status}

async def run_data_download(request: DataDownloadRequest):
    """åå°è¿è¡Œæ•°æ®ä¸‹è½½ä»»åŠ¡"""
    global data_status

    try:
        data_status.message = "Initializing download..."
        data_status.progress = 10.0

        # è¿™é‡Œå¯ä»¥è°ƒç”¨ç°æœ‰çš„bn_data.main.run()å‡½æ•°
        # æˆ–è€…é‡æ„ä¸ºæ›´ç»†ç²’åº¦çš„æ§åˆ¶
        data_status.message = "Downloading market data..."
        data_status.progress = 50.0

        # æ¨¡æ‹Ÿä¸‹è½½è¿‡ç¨‹
        await asyncio.sleep(2)

        data_status.message = "Processing data..."
        data_status.progress = 80.0

        await asyncio.sleep(1)

        data_status.status = "completed"
        data_status.progress = 100.0
        data_status.message = "Data download completed successfully"

    except Exception as e:
        data_status.status = "error"
        data_status.message = f"Download failed: {str(e)}"

@router.get("/market/{symbol}")
async def get_market_data(
    symbol: str,
    interval: str = "1H",
    limit: int = 1000,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
    """è·å–ç‰¹å®šå¸ç§çš„å¸‚åœºæ•°æ®"""
    try:
        logger.info(f"ğŸ” è·å–å¸‚åœºæ•°æ®: {symbol}, å‘¨æœŸ: {interval}, é™åˆ¶: {limit}")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨è·å–æ•°æ®
        df = data_adapter.get_symbol_data_for_backtest(symbol, start_date, end_date, interval)

        if df is None or df.empty:
            return {
                "symbol": symbol,
                "interval": interval,
                "data": [],
                "message": f"No data found for {symbol}",
                "status": "no_data"
            }

        # é™åˆ¶è¿”å›æ•°æ®é‡
        if limit > 0:
            df = df.tail(limit)  # è·å–æœ€æ–°çš„æ•°æ®

        # è½¬æ¢ä¸ºAPIè¿”å›æ ¼å¼
        data_list = []
        for _, row in df.iterrows():
            try:
                record = {
                    "timestamp": row['candle_begin_time'].isoformat() if pd.notna(row['candle_begin_time']) else None,
                    "open": float(row['open']) if pd.notna(row['open']) else 0.0,
                    "high": float(row['high']) if pd.notna(row['high']) else 0.0,
                    "low": float(row['low']) if pd.notna(row['low']) else 0.0,
                    "close": float(row['close']) if pd.notna(row['close']) else 0.0,
                    "volume": float(row['volume']) if pd.notna(row['volume']) else 0.0,
                }
                data_list.append(record)
            except Exception as e:
                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆæ•°æ®è¡Œ: {e}")
                continue

        logger.info(f"âœ… æˆåŠŸè·å– {len(data_list)} æ¡å¸‚åœºæ•°æ®")
        return {
            "symbol": symbol,
            "interval": interval,
            "data": data_list,
            "message": f"Successfully loaded {len(data_list)} records for {symbol}",
            "status": "success"
        }

    except Exception as e:
        logger.error(f"âŒ è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get market data: {str(e)}")

@router.delete("/cache")
async def clear_data_cache():
    """æ¸…ç†æ•°æ®ç¼“å­˜"""
    try:
        # å®ç°ç¼“å­˜æ¸…ç†é€»è¾‘
        return {"message": "Data cache cleared successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")


@router.get("/data-status")
async def get_data_status_summary():
    """è·å–æœ¬åœ°æ•°æ®çŠ¶æ€æ‘˜è¦"""
    try:
        logger.info("ğŸ“Š è·å–æ•°æ®çŠ¶æ€æ‘˜è¦")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨è·å–çŠ¶æ€
        status = data_adapter.get_data_status_summary()

        logger.info(f"âœ… æ•°æ®çŠ¶æ€: {status['status']}")
        return status

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®çŠ¶æ€å¤±è´¥: {e}")
        return {
            "data_source": "æœ¬åœ°é¢„å¤„ç†æ•°æ®",
            "status": "error",
            "error": str(e),
            "last_updated": datetime.now().isoformat()
        }

@router.get("/data-availability")
async def check_data_availability(
    symbols: str,  # é€—å·åˆ†éš”çš„äº¤æ˜“å¯¹åˆ—è¡¨
    start_date: str,
    end_date: str,
    min_records: int = 1000
):
    """æ£€æŸ¥æ•°æ®å¯ç”¨æ€§"""
    try:
        symbol_list = [s.strip() for s in symbols.split(',')]
        logger.info(f"ğŸ” æ£€æŸ¥æ•°æ®å¯ç”¨æ€§: {symbol_list}, {start_date} - {end_date}")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨æ£€æŸ¥å¯ç”¨æ€§
        availability = data_adapter.check_data_availability_for_backtest(
            symbol_list, start_date, end_date, min_records
        )

        logger.info(f"âœ… å¯ç”¨äº¤æ˜“å¯¹: {len(availability['available_symbols'])}/{len(symbol_list)}")
        return availability

    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥æ•°æ®å¯ç”¨æ€§å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to check data availability: {str(e)}")

@router.get("/intelligent-time-range")
async def get_intelligent_time_range(
    symbols: str,  # é€—å·åˆ†éš”çš„äº¤æ˜“å¯¹åˆ—è¡¨
    requested_start: str,
    requested_end: str,
    min_records: int = 1000
):
    """è·å–æ™ºèƒ½æ—¶é—´èŒƒå›´å»ºè®®"""
    try:
        symbol_list = [s.strip() for s in symbols.split(',')]
        logger.info(f"ğŸ§  è·å–æ™ºèƒ½æ—¶é—´èŒƒå›´: {symbol_list}, {requested_start} - {requested_end}")

        # ä½¿ç”¨æ•°æ®é€‚é…å™¨è·å–æ™ºèƒ½æ—¶é—´èŒƒå›´
        time_range_info = data_adapter.get_intelligent_time_range_for_backtest(
            symbol_list, requested_start, requested_end, min_records
        )

        logger.info(f"âœ… æ—¶é—´èŒƒå›´ä¼˜åŒ–: {time_range_info['success']}")
        return time_range_info

    except Exception as e:
        logger.error(f"âŒ è·å–æ™ºèƒ½æ—¶é—´èŒƒå›´å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get intelligent time range: {str(e)}")
